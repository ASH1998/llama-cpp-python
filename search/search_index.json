{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udd99 Python Bindings for <code>llama.cpp</code>","text":"<p>Simple Python bindings for @ggerganov's <code>llama.cpp</code> library. This package provides:</p> <ul> <li>Low-level access to C API via <code>ctypes</code> interface.</li> <li>High-level Python API for text completion</li> <li>OpenAI-like API</li> <li>LangChain compatibility</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install llama-cpp-python\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; llm = Llama(model_path=\"models/7B/...\")\n&gt;&gt;&gt; output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n&gt;&gt;&gt; print(output)\n{\n\"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\"object\": \"text_completion\",\n\"created\": 1679561337,\n\"model\": \"models/7B/...\",\n\"choices\": [\n{\n\"text\": \"Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\",\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": \"stop\"\n}\n],\n\"usage\": {\n\"prompt_tokens\": 14,\n\"completion_tokens\": 28,\n\"total_tokens\": 42\n}\n}\n</code></pre>"},{"location":"#development","title":"Development","text":"<pre><code>git clone git@github.com:abetlen/llama-cpp-python.git\ngit submodule update --init --recursive\n# Will need to be re-run any time vendor/llama.cpp is updated\npython3 setup.py develop\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":""},{"location":"#llama_cpp.Llama","title":"<code>llama_cpp.Llama</code>","text":"<p>High-level Python wrapper for a llama.cpp model.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>class Llama:\n\"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\ndef __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nseed: int = 1337,\nf16_kv: bool = False,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 8,\nlast_n_tokens_size: int = 64,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n        Args:\n            model_path: Path to the model.\n            n_ctx: Maximum context size.\n            n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n            seed: Random seed. 0 for random.\n            f16_kv: Use half-precision for key/value cache.\n            logits_all: Return logits for all tokens, not just the last token.\n            vocab_only: Only load the vocabulary no weights.\n            use_mlock: Force the system to keep the model in RAM.\n            embedding: Embedding mode only.\n            n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n            n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n            last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n        Raises:\n            ValueError: If the model path does not exist.\n        Returns:\n            A Llama instance.\n        \"\"\"\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_parts = n_parts\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.last_n_tokens_size = last_n_tokens_size\nself.last_n_tokens_data = deque(\n[llama_cpp.llama_token(0)] * self.last_n_tokens_size,\nmaxlen=self.last_n_tokens_size,\n)\nself.tokens_consumed = 0\nself.n_batch = n_batch\nself.n_threads = n_threads or multiprocessing.cpu_count()\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\ndef tokenize(self, text: bytes) -&gt; List[llama_cpp.llama_token]:\n\"\"\"Tokenize a string.\n        Args:\n            text: The utf-8 encoded string to tokenize.\n        Raises:\n            RuntimeError: If the tokenization failed.\n        Returns:\n            A list of tokens.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = llama_cpp.llama_n_ctx(self.ctx)\ntokens = (llama_cpp.llama_token * int(n_ctx))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nn_ctx,\nllama_cpp.c_bool(True),\n)\nif int(n_tokens) &lt; 0:\nraise RuntimeError(f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}')\nreturn list(tokens[:n_tokens])\ndef detokenize(self, tokens: List[llama_cpp.llama_token]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n        Args:\n            tokens: The list of tokens to detokenize.\n        Returns:\n            The detokenized string.\n        \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(self.ctx, token)\nreturn output\ndef reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.last_n_tokens_data.extend(\n[llama_cpp.llama_token(0)] * self.last_n_tokens_size\n)\nself.tokens_consumed = 0\ndef eval(self, tokens: Sequence[llama_cpp.llama_token]):\n\"\"\"Evaluate a list of tokens.\n        Args:\n            tokens: The list of tokens to evaluate.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), self.tokens_consumed)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(len(batch)),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif int(return_code) != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\nself.last_n_tokens_data.extend(batch)\nself.tokens_consumed += len(batch)\ndef sample(\nself,\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\n):\n\"\"\"Sample a token from the model.\n        Args:\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n        Returns:\n            The sampled token.\n        \"\"\"\nassert self.ctx is not None\n# Temporary workaround for https://github.com/ggerganov/llama.cpp/issues/684\nif temp == 0.0:\ntemp = 1.0\ntop_p = 0.0\ntop_k = 1\nreturn llama_cpp.llama_sample_top_p_top_k(\nctx=self.ctx,\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*self.last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\n)\ndef generate(\nself,\ntokens: Sequence[llama_cpp.llama_token],\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\n) -&gt; Generator[\nllama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None\n]:\n\"\"\"Create a generator of tokens from a prompt.\n        Examples:\n            &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n            &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n            &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n            ...     print(llama.detokenize([token]))\n        Args:\n            tokens: The prompt tokens.\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n        Yields:\n            The generated tokens.\n        \"\"\"\nassert self.ctx is not None\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\n)\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\ndef create_embedding(self, input: str) -&gt; Embedding:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            An embedding object.\n        \"\"\"\nassert self.ctx is not None\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\nreturn {\n\"object\": \"list\",\n\"data\": [\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": 0,\n}\n],\n\"model\": self.model_path,\n\"usage\": {\n\"prompt_tokens\": n_tokens,\n\"total_tokens\": n_tokens,\n},\n}\ndef _create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 16,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: List[str] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[\nGenerator[Completion, None, None],\nGenerator[CompletionChunk, None, None],\n]:\nassert self.ctx is not None\ncompletion_id = f\"cmpl-{str(uuid.uuid4())}\"\ncreated = int(time.time())\ncompletion_tokens: List[llama_cpp.llama_token] = []\n# Add blank space to start of prompt to match OG llama tokenizer\nprompt_tokens = self.tokenize(b\" \" + prompt.encode(\"utf-8\"))\ntext = b\"\"\nreturned_characters = 0\nif len(prompt_tokens) + max_tokens &gt; int(llama_cpp.llama_n_ctx(self.ctx)):\nraise ValueError(\nf\"Requested tokens exceed context window of {llama_cpp.llama_n_ctx(self.ctx)}\"\n)\nif stop != []:\nstop_sequences = [s.encode(\"utf-8\") for s in stop]\nelse:\nstop_sequences = []\nfinish_reason = None\nfor token in self.generate(\nprompt_tokens,\ntop_k=top_k,\ntop_p=top_p,\ntemp=temperature,\nrepeat_penalty=repeat_penalty,\n):\nif token == llama_cpp.llama_token_eos():\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"stop\"\nbreak\ncompletion_tokens.append(token)\nall_text = self.detokenize(completion_tokens)\nany_stop = [s for s in stop_sequences if s in all_text]\nif len(any_stop) &gt; 0:\nfirst_stop = any_stop[0]\ntext = all_text[: all_text.index(first_stop)]\nfinish_reason = \"stop\"\nbreak\nif stream:\nstart = returned_characters\nlongest = 0\n# We want to avoid yielding any characters from\n# the generated text if they are part of a stop\n# sequence.\nfor s in stop_sequences:\nfor i in range(len(s), 0, -1):\nif all_text.endswith(s[:i]):\nif i &gt; longest:\nlongest = i\nbreak\ntext = all_text[: len(all_text) - longest]\nreturned_characters += len(text[start:])\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": self.model_path,\n\"choices\": [\n{\n\"text\": text[start :].decode(\"utf-8\"),\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": None,\n}\n],\n}\nif len(completion_tokens) &gt;= max_tokens:\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"length\"\nbreak\nif finish_reason is None:\nfinish_reason = \"length\"\nif stream:\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": self.model_path,\n\"choices\": [\n{\n\"text\": text[returned_characters:].decode(\"utf-8\"),\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": finish_reason,\n}\n],\n}\nreturn\ntext = text.decode(\"utf-8\")\nif echo:\ntext = prompt + text\nif suffix is not None:\ntext = text + suffix\nif logprobs is not None:\nraise NotImplementedError(\"logprobs not implemented\")\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": self.model_path,\n\"choices\": [\n{\n\"text\": text,\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": finish_reason,\n}\n],\n\"usage\": {\n\"prompt_tokens\": len(prompt_tokens),\n\"completion_tokens\": len(completion_tokens),\n\"total_tokens\": len(prompt_tokens) + len(completion_tokens),\n},\n}\ndef create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: List[str] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Completion, Generator[CompletionChunk, None, None]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\nif stream:\nchunks: Generator[CompletionChunk, None, None] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\ndef __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: List[str] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n):\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\ndef __del__(self):\nif self.ctx is not None:\nllama_cpp.llama_free(self.ctx)\nself.ctx = None\n@staticmethod\ndef token_eos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n@staticmethod\ndef token_bos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__init__","title":"<code>__init__(model_path, n_ctx=512, n_parts=-1, seed=1337, f16_kv=False, logits_all=False, vocab_only=False, use_mlock=False, embedding=False, n_threads=None, n_batch=8, last_n_tokens_size=64)</code>","text":"<p>Load a llama.cpp model from <code>model_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>n_ctx</code> <code>int</code> <p>Maximum context size.</p> <code>512</code> <code>n_parts</code> <code>int</code> <p>Number of parts to split the model into. If -1, the number of parts is automatically determined.</p> <code>-1</code> <code>seed</code> <code>int</code> <p>Random seed. 0 for random.</p> <code>1337</code> <code>f16_kv</code> <code>bool</code> <p>Use half-precision for key/value cache.</p> <code>False</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens, not just the last token.</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>Only load the vocabulary no weights.</p> <code>False</code> <code>use_mlock</code> <code>bool</code> <p>Force the system to keep the model in RAM.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Embedding mode only.</p> <code>False</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads to use. If None, the number of threads is automatically determined.</p> <code>None</code> <code>n_batch</code> <code>int</code> <p>Maximum number of prompt tokens to batch together when calling llama_eval.</p> <code>8</code> <code>last_n_tokens_size</code> <code>int</code> <p>Maximum number of tokens to keep in the last_n_tokens deque.</p> <code>64</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model path does not exist.</p> <p>Returns:</p> Type Description <p>A Llama instance.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nseed: int = 1337,\nf16_kv: bool = False,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 8,\nlast_n_tokens_size: int = 64,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n    Args:\n        model_path: Path to the model.\n        n_ctx: Maximum context size.\n        n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n        seed: Random seed. 0 for random.\n        f16_kv: Use half-precision for key/value cache.\n        logits_all: Return logits for all tokens, not just the last token.\n        vocab_only: Only load the vocabulary no weights.\n        use_mlock: Force the system to keep the model in RAM.\n        embedding: Embedding mode only.\n        n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n        n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n        last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n    Raises:\n        ValueError: If the model path does not exist.\n    Returns:\n        A Llama instance.\n    \"\"\"\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_parts = n_parts\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.last_n_tokens_size = last_n_tokens_size\nself.last_n_tokens_data = deque(\n[llama_cpp.llama_token(0)] * self.last_n_tokens_size,\nmaxlen=self.last_n_tokens_size,\n)\nself.tokens_consumed = 0\nself.n_batch = n_batch\nself.n_threads = n_threads or multiprocessing.cpu_count()\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenize a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>bytes</code> <p>The utf-8 encoded string to tokenize.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the tokenization failed.</p> <p>Returns:</p> Type Description <code>List[llama_cpp.llama_token]</code> <p>A list of tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def tokenize(self, text: bytes) -&gt; List[llama_cpp.llama_token]:\n\"\"\"Tokenize a string.\n    Args:\n        text: The utf-8 encoded string to tokenize.\n    Raises:\n        RuntimeError: If the tokenization failed.\n    Returns:\n        A list of tokens.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = llama_cpp.llama_n_ctx(self.ctx)\ntokens = (llama_cpp.llama_token * int(n_ctx))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nn_ctx,\nllama_cpp.c_bool(True),\n)\nif int(n_tokens) &lt; 0:\nraise RuntimeError(f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}')\nreturn list(tokens[:n_tokens])\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.detokenize","title":"<code>detokenize(tokens)</code>","text":"<p>Detokenize a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[llama_cpp.llama_token]</code> <p>The list of tokens to detokenize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The detokenized string.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def detokenize(self, tokens: List[llama_cpp.llama_token]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n    Args:\n        tokens: The list of tokens to detokenize.\n    Returns:\n        The detokenized string.\n    \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(self.ctx, token)\nreturn output\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.reset","title":"<code>reset()</code>","text":"<p>Reset the model state.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.last_n_tokens_data.extend(\n[llama_cpp.llama_token(0)] * self.last_n_tokens_size\n)\nself.tokens_consumed = 0\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.eval","title":"<code>eval(tokens)</code>","text":"<p>Evaluate a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[llama_cpp.llama_token]</code> <p>The list of tokens to evaluate.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def eval(self, tokens: Sequence[llama_cpp.llama_token]):\n\"\"\"Evaluate a list of tokens.\n    Args:\n        tokens: The list of tokens to evaluate.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), self.tokens_consumed)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(len(batch)),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif int(return_code) != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\nself.last_n_tokens_data.extend(batch)\nself.tokens_consumed += len(batch)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.sample","title":"<code>sample(top_k, top_p, temp, repeat_penalty)</code>","text":"<p>Sample a token from the model.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> required <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> required <code>temp</code> <code>float</code> <p>The temperature parameter.</p> required <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> required <p>Returns:</p> Type Description <p>The sampled token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def sample(\nself,\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\n):\n\"\"\"Sample a token from the model.\n    Args:\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n    Returns:\n        The sampled token.\n    \"\"\"\nassert self.ctx is not None\n# Temporary workaround for https://github.com/ggerganov/llama.cpp/issues/684\nif temp == 0.0:\ntemp = 1.0\ntop_p = 0.0\ntop_k = 1\nreturn llama_cpp.llama_sample_top_p_top_k(\nctx=self.ctx,\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*self.last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.generate","title":"<code>generate(tokens, top_k, top_p, temp, repeat_penalty)</code>","text":"<p>Create a generator of tokens from a prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n&gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n&gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n...     print(llama.detokenize([token]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[llama_cpp.llama_token]</code> <p>The prompt tokens.</p> required <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> required <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> required <code>temp</code> <code>float</code> <p>The temperature parameter.</p> required <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> required <p>Yields:</p> Type Description <code>Generator[llama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None]</code> <p>The generated tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def generate(\nself,\ntokens: Sequence[llama_cpp.llama_token],\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\n) -&gt; Generator[\nllama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None\n]:\n\"\"\"Create a generator of tokens from a prompt.\n    Examples:\n        &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n        &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n        &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n        ...     print(llama.detokenize([token]))\n    Args:\n        tokens: The prompt tokens.\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n    Yields:\n        The generated tokens.\n    \"\"\"\nassert self.ctx is not None\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\n)\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_embedding","title":"<code>create_embedding(input)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>An embedding object.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_embedding(self, input: str) -&gt; Embedding:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        An embedding object.\n    \"\"\"\nassert self.ctx is not None\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\nreturn {\n\"object\": \"list\",\n\"data\": [\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": 0,\n}\n],\n\"model\": self.model_path,\n\"usage\": {\n\"prompt_tokens\": n_tokens,\n\"total_tokens\": n_tokens,\n},\n}\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_completion","title":"<code>create_completion(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], repeat_penalty=1.1, top_k=40, stream=False)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>List[str]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Generator[CompletionChunk, None, None]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: List[str] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Completion, Generator[CompletionChunk, None, None]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\nif stream:\nchunks: Generator[CompletionChunk, None, None] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__call__","title":"<code>__call__(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], repeat_penalty=1.1, top_k=40, stream=False)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>List[str]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: List[str] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n):\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_bos","title":"<code>token_bos()</code>  <code>staticmethod</code>","text":"<p>Return the beginning-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_bos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_eos","title":"<code>token_eos()</code>  <code>staticmethod</code>","text":"<p>Return the end-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_eos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_context_p","title":"<code>llama_context_p = c_void_p</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params_p","title":"<code>llama_context_params_p = POINTER(llama_context_params)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_progress_callback","title":"<code>llama_progress_callback = ctypes.CFUNCTYPE(None, c_float, c_void_p)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token","title":"<code>llama_token = c_int</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_data_p","title":"<code>llama_token_data_p = POINTER(llama_token_data)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_p","title":"<code>llama_token_p = POINTER(llama_token)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params","title":"<code>llama_context_params</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_context_params(Structure):\n_fields_ = [\n(\"n_ctx\", c_int),  # text context\n(\"n_parts\", c_int),  # -1 for default\n(\"seed\", c_int),  # RNG seed, 0 for random\n(\"f16_kv\", c_bool),  # use fp16 for KV cache\n(\n\"logits_all\",\nc_bool,\n),  # the llama_eval() call computes all logits, not just the last one\n(\"vocab_only\", c_bool),  # only load the vocabulary, no weights\n(\"use_mlock\", c_bool),  # force system to keep model in RAM\n(\"embedding\", c_bool),  # embedding mode only\n# called with a progress value between 0 and 1, pass NULL to disable\n(\"progress_callback\", llama_progress_callback),\n# context pointer passed to the progress callback\n(\"progress_callback_user_data\", c_void_p),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_data","title":"<code>llama_token_data</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_token_data(Structure):\n_fields_ = [\n(\"id\", llama_token),  # token id\n(\"p\", c_float),  # probability of the token\n(\"plog\", c_float),  # log probability of the token\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_context_default_params","title":"<code>llama_context_default_params()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_context_default_params() -&gt; llama_context_params:\nreturn _lib.llama_context_default_params()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_eval","title":"<code>llama_eval(ctx, tokens, n_tokens, n_past, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_eval(\nctx: llama_context_p,\ntokens,  # type: Array[llama_token]\nn_tokens: c_int,\nn_past: c_int,\nn_threads: c_int,\n) -&gt; c_int:\nreturn _lib.llama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_free","title":"<code>llama_free(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_free(ctx: llama_context_p):\n_lib.llama_free(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_embeddings","title":"<code>llama_get_embeddings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_embeddings(ctx: llama_context_p):\nreturn _lib.llama_get_embeddings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_kv_cache","title":"<code>llama_get_kv_cache(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_kv_cache(ctx: llama_context_p):\nreturn _lib.llama_get_kv_cache(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_kv_cache_size","title":"<code>llama_get_kv_cache_size(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_kv_cache_size(ctx: llama_context_p) -&gt; c_size_t:\nreturn _lib.llama_get_kv_cache_size(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_kv_cache_token_count","title":"<code>llama_get_kv_cache_token_count(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_kv_cache_token_count(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_get_kv_cache_token_count(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_logits","title":"<code>llama_get_logits(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_logits(ctx: llama_context_p):\nreturn _lib.llama_get_logits(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_init_from_file","title":"<code>llama_init_from_file(path_model, params)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_init_from_file(\npath_model: bytes, params: llama_context_params\n) -&gt; llama_context_p:\nreturn _lib.llama_init_from_file(path_model, params)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_model_quantize","title":"<code>llama_model_quantize(fname_inp, fname_out, itype, qk)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_model_quantize(\nfname_inp: bytes, fname_out: bytes, itype: c_int, qk: c_int\n) -&gt; c_int:\nreturn _lib.llama_model_quantize(fname_inp, fname_out, itype, qk)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_ctx","title":"<code>llama_n_ctx(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_ctx(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_ctx(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_embd","title":"<code>llama_n_embd(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_embd(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_ctx(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_vocab","title":"<code>llama_n_vocab(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_vocab(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_vocab(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_system_info","title":"<code>llama_print_system_info()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_system_info() -&gt; bytes:\nreturn _lib.llama_print_system_info()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_timings","title":"<code>llama_print_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_timings(ctx: llama_context_p):\n_lib.llama_print_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_reset_timings","title":"<code>llama_reset_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_reset_timings(ctx: llama_context_p):\n_lib.llama_reset_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_top_p_top_k","title":"<code>llama_sample_top_p_top_k(ctx, last_n_tokens_data, last_n_tokens_size, top_k, top_p, temp, repeat_penalty)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_top_p_top_k(\nctx: llama_context_p,\nlast_n_tokens_data,  # type: Array[llama_token]\nlast_n_tokens_size: c_int,\ntop_k: c_int,\ntop_p: c_float,\ntemp: c_float,\nrepeat_penalty: c_float,\n) -&gt; llama_token:\nreturn _lib.llama_sample_top_p_top_k(\nctx, last_n_tokens_data, last_n_tokens_size, top_k, top_p, temp, repeat_penalty\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_kv_cache","title":"<code>llama_set_kv_cache(ctx, kv_cache, n_size, n_token_count)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_kv_cache(ctx: llama_context_p, kv_cache, n_size: c_size_t, n_token_count: c_int):\nreturn _lib.llama_set_kv_cache(ctx, kv_cache, n_size, n_token_count)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_bos","title":"<code>llama_token_bos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_bos() -&gt; llama_token:\nreturn _lib.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_eos","title":"<code>llama_token_eos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_eos() -&gt; llama_token:\nreturn _lib.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_to_str","title":"<code>llama_token_to_str(ctx, token)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_to_str(ctx: llama_context_p, token: llama_token) -&gt; bytes:\nreturn _lib.llama_token_to_str(ctx, token)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_tokenize","title":"<code>llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_tokenize(\nctx: llama_context_p,\ntext: bytes,\ntokens,  # type: Array[llama_token]\nn_max_tokens: c_int,\nadd_bos: c_bool,\n) -&gt; c_int:\nreturn _lib.llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"}]}