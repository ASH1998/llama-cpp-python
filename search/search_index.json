{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#python-bindings-for-llamacpp","title":"\ud83e\udd99 Python Bindings for <code>llama.cpp</code>","text":"<p>Simple Python bindings for @ggerganov's <code>llama.cpp</code> library. This package provides:</p> <ul> <li>Low-level access to C API via <code>ctypes</code> interface.</li> <li>High-level Python API for text completion</li> <li>OpenAI-like API</li> <li>LangChain compatibility</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install llama-cpp-python\n</code></pre>"},{"location":"#high-level-api","title":"High-level API","text":"<pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; llm = Llama(model_path=\"./models/7B/ggml-model.bin\")\n&gt;&gt;&gt; output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n&gt;&gt;&gt; print(output)\n{\n\"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\"object\": \"text_completion\",\n\"created\": 1679561337,\n\"model\": \"./models/7B/ggml-model.bin\",\n\"choices\": [\n{\n\"text\": \"Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\",\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": \"stop\"\n}\n],\n\"usage\": {\n\"prompt_tokens\": 14,\n\"completion_tokens\": 28,\n\"total_tokens\": 42\n}\n}\n</code></pre>"},{"location":"#web-server","title":"Web Server","text":"<p><code>llama-cpp-python</code> offers a web server which aims to act as a drop-in replacement for the OpenAI API. This allows you to use llama.cpp compatible models with any OpenAI compatible client (language libraries, services, etc).</p> <p>To install the server package and get started:</p> <pre><code>pip install llama-cpp-python[server]\nexport MODEL=./models/7B/ggml-model.bin\npython3 -m llama_cpp.server\n</code></pre> <p>Navigate to http://localhost:8000/docs to see the OpenAPI documentation.</p>"},{"location":"#low-level-api","title":"Low-level API","text":"<p>The low-level API is a direct <code>ctypes</code> binding to the C API provided by <code>llama.cpp</code>. The entire API can be found in llama_cpp/llama_cpp.py and should mirror llama.h.</p>"},{"location":"#development","title":"Development","text":"<p>This package is under active development and I welcome any contributions.</p> <p>To get started, clone the repository and install the package in development mode:</p> <pre><code>git clone git@github.com:abetlen/llama-cpp-python.git\ngit submodule update --init --recursive\n# Will need to be re-run any time vendor/llama.cpp is updated\npython3 setup.py develop\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":""},{"location":"#llama_cpp.Llama","title":"<code>llama_cpp.Llama</code>","text":"<p>High-level Python wrapper for a llama.cpp model.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>class Llama:\n\"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\ndef __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nn_gpu_layers: int = 0,\nseed: int = 1337,\nf16_kv: bool = True,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mmap: bool = True,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 512,\nlast_n_tokens_size: int = 64,\nlora_base: Optional[str] = None,\nlora_path: Optional[str] = None,\nlow_vram: bool = False,\nverbose: bool = True,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n        Args:\n            model_path: Path to the model.\n            n_ctx: Maximum context size.\n            n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n            seed: Random seed. 0 for random.\n            f16_kv: Use half-precision for key/value cache.\n            logits_all: Return logits for all tokens, not just the last token.\n            vocab_only: Only load the vocabulary no weights.\n            use_mmap: Use mmap if possible.\n            use_mlock: Force the system to keep the model in RAM.\n            embedding: Embedding mode only.\n            n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n            n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n            last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n            lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n            lora_path: Path to a LoRA file to apply to the model.\n            verbose: Print verbose output to stderr.\n        Raises:\n            ValueError: If the model path does not exist.\n        Returns:\n            A Llama instance.\n        \"\"\"\nself.verbose = verbose\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_gpu_layers = n_gpu_layers\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mmap = use_mmap if lora_path is None else False\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.params.low_vram = low_vram\nself.last_n_tokens_size = last_n_tokens_size\nself.n_batch = min(n_ctx, n_batch)\nself.eval_tokens: Deque[int] = deque(maxlen=n_ctx)\nself.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx if logits_all else 1)\nself.cache: Optional[BaseLlamaCache] = None\nself.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\nself.lora_base = lora_base\nself.lora_path = lora_path\n### DEPRECATED ###\nself.n_parts = n_parts\n### DEPRECATED ###\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\nassert self.ctx is not None\nif self.lora_path:\nif llama_cpp.llama_apply_lora_from_file(\nself.ctx,\nllama_cpp.c_char_p(self.lora_path.encode(\"utf-8\")),\nllama_cpp.c_char_p(self.lora_base.encode(\"utf-8\"))\nif self.lora_base is not None\nelse llama_cpp.c_char_p(0),\nllama_cpp.c_int(self.n_threads),\n):\nraise RuntimeError(\nf\"Failed to apply LoRA from lora path: {self.lora_path} to base path: {self.lora_base}\"\n)\nif self.verbose:\nprint(llama_cpp.llama_print_system_info().decode(\"utf-8\"), file=sys.stderr)\nself._n_vocab = self.n_vocab()\nself._n_ctx = self.n_ctx()\nsize = llama_cpp.c_size_t(self._n_vocab)\nsorted = llama_cpp.c_bool(False)\nself._candidates_data = np.array(\n[],\ndtype=np.dtype(\n[(\"id\", np.intc), (\"logit\", np.single), (\"p\", np.single)], align=True\n),\n)\nself._candidates_data.resize(3, self._n_vocab, refcheck=False)\ncandidates = llama_cpp.llama_token_data_array(\ndata=self._candidates_data.ctypes.data_as(llama_cpp.llama_token_data_p),\nsize=size,\nsorted=sorted,\n)\nself._candidates = candidates\nself._token_nl = Llama.token_nl()\nself._token_eos = Llama.token_eos()\nself._input_ids = np.array([], dtype=np.intc)\nself._scores = np.ndarray((0, self._n_vocab), dtype=np.single)\ndef tokenize(self, text: bytes, add_bos: bool = True) -&gt; List[int]:\n\"\"\"Tokenize a string.\n        Args:\n            text: The utf-8 encoded string to tokenize.\n        Raises:\n            RuntimeError: If the tokenization failed.\n        Returns:\n            A list of tokens.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = self._n_ctx\ntokens = (llama_cpp.llama_token * n_ctx)()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nllama_cpp.c_int(n_ctx),\nllama_cpp.c_bool(add_bos),\n)\nif n_tokens &lt; 0:\nn_tokens = abs(n_tokens)\ntokens = (llama_cpp.llama_token * n_tokens)()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nllama_cpp.c_int(n_tokens),\nllama_cpp.c_bool(add_bos),\n)\nif n_tokens &lt; 0:\nraise RuntimeError(\nf'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}'\n)\nreturn list(tokens[:n_tokens])\ndef detokenize(self, tokens: List[int]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n        Args:\n            tokens: The list of tokens to detokenize.\n        Returns:\n            The detokenized string.\n        \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(\nself.ctx, llama_cpp.llama_token(token)\n)\nreturn output\ndef set_cache(self, cache: Optional[BaseLlamaCache]):\n\"\"\"Set the cache.\n        Args:\n            cache: The cache to set.\n        \"\"\"\nself.cache = cache\ndef reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.eval_tokens.clear()\nself.eval_logits.clear()\nself._input_ids = np.array([], dtype=np.intc)\nself._scores = np.ndarray((0, self._n_vocab), dtype=np.single)\ndef eval(self, tokens: Sequence[int]):\n\"\"\"Evaluate a list of tokens.\n        Args:\n            tokens: The list of tokens to evaluate.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = self._n_ctx\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), len(self._input_ids))\nn_tokens = len(batch)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(n_tokens),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif return_code != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\n# Save tokens\nself.eval_tokens.extend(batch)\nself._input_ids: npt.NDArray[np.intc] = np.concatenate(\n(self._input_ids, np.array(batch, dtype=np.intc)), axis=0\n)\n# Save logits\nrows = n_tokens if self.params.logits_all else 1\nn_vocab = self._n_vocab\ncols = n_vocab\nlogits_view = llama_cpp.llama_get_logits(self.ctx)\nlogits = [logits_view[i * cols : (i + 1) * cols] for i in range(rows)]\nself.eval_logits.extend(logits)\nself._scores: npt.NDArray[np.single] = np.concatenate(\n(self._scores, np.array(logits, dtype=np.single)), axis=0\n)\ndef _sample(\nself,\nlast_n_tokens_data,  # type: llama_cpp.Array[llama_cpp.llama_token]\nlast_n_tokens_size: llama_cpp.c_int,\ntop_k: llama_cpp.c_int,\ntop_p: llama_cpp.c_float,\ntemp: llama_cpp.c_float,\ntfs_z: llama_cpp.c_float,\nrepeat_penalty: llama_cpp.c_float,\nfrequency_penalty: llama_cpp.c_float,\npresence_penalty: llama_cpp.c_float,\nmirostat_mode: llama_cpp.c_int,\nmirostat_tau: llama_cpp.c_float,\nmirostat_eta: llama_cpp.c_float,\npenalize_nl: bool = True,\nlogits_processor: Optional[LogitsProcessorList] = None,\n):\nassert self.ctx is not None\nassert len(self.eval_logits) &gt; 0\nassert self._scores.shape[0] &gt; 0\nn_vocab = self._n_vocab\nn_ctx = self._n_ctx\ntop_k = llama_cpp.c_int(n_vocab) if top_k.value &lt;= 0 else top_k\nlast_n_tokens_size = (\nllama_cpp.c_int(n_ctx)\nif last_n_tokens_size.value &lt; 0\nelse last_n_tokens_size\n)\nlogits: npt.NDArray[np.single] = self._scores[-1, :]\nif logits_processor is not None:\nlogits = np.array(\nlogits_processor(self._input_ids.tolist(), logits.tolist()),\ndtype=np.single,\n)\nself._scores[-1, :] = logits\nself.eval_logits[-1] = logits.tolist()\nnl_logit = logits[self._token_nl]\ncandidates = self._candidates\ncandidates_data = self._candidates_data\ncandidates_data[\"id\"] = np.arange(n_vocab, dtype=np.intc)  # type: ignore\ncandidates_data[\"logit\"] = logits\ncandidates_data[\"p\"] = np.zeros(n_vocab, dtype=np.single)\ncandidates.data = candidates_data.ctypes.data_as(llama_cpp.llama_token_data_p)\ncandidates.sorted = llama_cpp.c_bool(False)\ncandidates.size = llama_cpp.c_size_t(n_vocab)\nllama_cpp.llama_sample_repetition_penalty(\nctx=self.ctx,\nlast_tokens_data=last_n_tokens_data,\nlast_tokens_size=last_n_tokens_size,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\npenalty=repeat_penalty,\n)\nllama_cpp.llama_sample_frequency_and_presence_penalties(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\nlast_tokens_data=last_n_tokens_data,\nlast_tokens_size=last_n_tokens_size,\nalpha_frequency=frequency_penalty,\nalpha_presence=presence_penalty,\n)\nif not penalize_nl:\ncandidates.data[self._token_nl].logit = llama_cpp.c_float(nl_logit)\nif temp.value == 0.0:\nreturn llama_cpp.llama_sample_token_greedy(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\n)\nelif mirostat_mode.value == 1:\nmirostat_mu = llama_cpp.c_float(2.0 * mirostat_tau.value)\nmirostat_m = llama_cpp.c_int(100)\nllama_cpp.llama_sample_temperature(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntemp=temp,\n)\nreturn llama_cpp.llama_sample_token_mirostat(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntau=mirostat_tau,\neta=mirostat_eta,\nmu=llama_cpp.ctypes.byref(mirostat_mu),  # type: ignore\nm=mirostat_m,\n)\nelif mirostat_mode.value == 2:\nmirostat_mu = llama_cpp.c_float(2.0 * mirostat_tau.value)\nllama_cpp.llama_sample_temperature(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.pointer(candidates),\ntemp=temp,\n)\nreturn llama_cpp.llama_sample_token_mirostat_v2(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntau=mirostat_tau,\neta=mirostat_eta,\nmu=llama_cpp.ctypes.byref(mirostat_mu),  # type: ignore\n)\nelse:\nllama_cpp.llama_sample_top_k(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\nk=top_k,\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_tail_free(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\nz=tfs_z,\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_typical(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\np=llama_cpp.c_float(1.0),\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_top_p(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\np=top_p,\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_temperature(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntemp=temp,\n)\nreturn llama_cpp.llama_sample_token(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\n)\ndef sample(\nself,\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_eta: float = 0.1,\nmirostat_tau: float = 5.0,\npenalize_nl: bool = True,\nlogits_processor: Optional[LogitsProcessorList] = None,\n):\n\"\"\"Sample a token from the model.\n        Args:\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n        Returns:\n            The sampled token.\n        \"\"\"\nassert self.ctx is not None\nlast_n_tokens_data = [llama_cpp.llama_token(0)] * max(\n0, self.last_n_tokens_size - len(self._input_ids)\n) + self._input_ids[-self.last_n_tokens_size :].tolist()\nreturn self._sample(\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\ntfs_z=llama_cpp.c_float(tfs_z),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\nfrequency_penalty=llama_cpp.c_float(frequency_penalty),\npresence_penalty=llama_cpp.c_float(presence_penalty),\nmirostat_mode=llama_cpp.c_int(mirostat_mode),\nmirostat_tau=llama_cpp.c_float(mirostat_tau),\nmirostat_eta=llama_cpp.c_float(mirostat_eta),\npenalize_nl=penalize_nl,\nlogits_processor=logits_processor,\n)\ndef generate(\nself,\ntokens: Sequence[int],\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nreset: bool = True,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nlogits_processor: Optional[LogitsProcessorList] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\n) -&gt; Generator[int, Optional[Sequence[int]], None]:\n\"\"\"Create a generator of tokens from a prompt.\n        Examples:\n            &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n            &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n            &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n            ...     print(llama.detokenize([token]))\n        Args:\n            tokens: The prompt tokens.\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n            reset: Whether to reset the model state.\n        Yields:\n            The generated tokens.\n        \"\"\"\nassert self.ctx is not None\nif reset and len(self._input_ids) &gt; 0:\nlongest_prefix = 0\nfor a, b in zip(self._input_ids, tokens[:-1]):\nif a == b:\nlongest_prefix += 1\nelse:\nbreak\nif longest_prefix &gt; 0:\nif self.verbose:\nprint(\"Llama.generate: prefix-match hit\", file=sys.stderr)\nreset = False\ntokens = tokens[longest_prefix:]\nself._input_ids = self._input_ids[:longest_prefix]\nself._scores = self._scores[:longest_prefix, :]\nfor _ in range(len(self.eval_tokens) - longest_prefix):\nself.eval_tokens.pop()\ntry:\nself.eval_logits.pop()\nexcept IndexError:\npass\nif reset:\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nlogits_processor=logits_processor,\n)\nif stopping_criteria is not None and stopping_criteria(\nself._input_ids.tolist(), self._scores[-1, :].tolist()\n):\nreturn\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\ndef create_embedding(\nself, input: Union[str, List[str]], model: Optional[str] = None\n) -&gt; Embedding:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            An embedding object.\n        \"\"\"\nassert self.ctx is not None\nmodel_name: str = model if model is not None else self.model_path\nif self.params.embedding == False:\nraise RuntimeError(\n\"Llama model must be created with embedding=True to call this method\"\n)\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\nif isinstance(input, str):\ninputs = [input]\nelse:\ninputs = input\ndata: List[EmbeddingData] = []\ntotal_tokens = 0\nfor index, input in enumerate(inputs):\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\ntotal_tokens += n_tokens\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\ndata.append(\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": index,\n}\n)\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nreturn {\n\"object\": \"list\",\n\"data\": data,\n\"model\": model_name,\n\"usage\": {\n\"prompt_tokens\": total_tokens,\n\"total_tokens\": total_tokens,\n},\n}\ndef embed(self, input: str) -&gt; List[float]:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            A list of embeddings\n        \"\"\"\nreturn list(map(float, self.create_embedding(input)[\"data\"][0][\"embedding\"]))\ndef _create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 16,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[Iterator[Completion], Iterator[CompletionChunk]]:\nassert self.ctx is not None\ncompletion_id: str = f\"cmpl-{str(uuid.uuid4())}\"\ncreated: int = int(time.time())\ncompletion_tokens: List[int] = []\n# Add blank space to start of prompt to match OG llama tokenizer\nprompt_tokens: List[int] = self.tokenize(b\" \" + prompt.encode(\"utf-8\"))\ntext: bytes = b\"\"\nreturned_tokens: int = 0\nstop = (\nstop if isinstance(stop, list) else [stop] if isinstance(stop, str) else []\n)\nmodel_name: str = model if model is not None else self.model_path\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\nif len(prompt_tokens) &gt; self._n_ctx:\nraise ValueError(f\"Requested tokens exceed context window of {self._n_ctx}\")\n# Truncate max_tokens if requested tokens would exceed the context window\nmax_tokens = (\nmax_tokens\nif max_tokens + len(prompt_tokens) &lt; self._n_ctx\nelse (self._n_ctx - len(prompt_tokens))\n)\nif stop != []:\nstop_sequences = [s.encode(\"utf-8\") for s in stop]\nelse:\nstop_sequences = []\nif logprobs is not None and self.params.logits_all is False:\nraise ValueError(\n\"logprobs is not supported for models created with logits_all=False\"\n)\nif self.cache:\ntry:\ncache_item = self.cache[prompt_tokens]\ncache_prefix_len = Llama.longest_token_prefix(\ncache_item.input_ids.tolist(), prompt_tokens\n)\neval_prefix_len = Llama.longest_token_prefix(\nself._input_ids.tolist(), prompt_tokens\n)\nif cache_prefix_len &gt; eval_prefix_len:\nself.load_state(cache_item)\nif self.verbose:\nprint(\"Llama._create_completion: cache hit\", file=sys.stderr)\nexcept KeyError:\nif self.verbose:\nprint(\"Llama._create_completion: cache miss\", file=sys.stderr)\nfinish_reason = \"length\"\nmultibyte_fix = 0\nfor token in self.generate(\nprompt_tokens,\ntop_k=top_k,\ntop_p=top_p,\ntemp=temperature,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\nstopping_criteria=stopping_criteria,\nlogits_processor=logits_processor,\n):\nif token == self._token_eos:\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"stop\"\nbreak\ncompletion_tokens.append(token)\nall_text = self.detokenize(completion_tokens)\n# Contains multi-byte UTF8\nfor k, char in enumerate(all_text[-3:]):\nk = 3 - k\nfor num, pattern in [(2, 192), (3, 224), (4, 240)]:\n# Bitwise AND check\nif num &gt; k and pattern &amp; char == pattern:\nmultibyte_fix = num - k\n# Stop incomplete bytes from passing\nif multibyte_fix &gt; 0:\nmultibyte_fix -= 1\ncontinue\nany_stop = [s for s in stop_sequences if s in all_text]\nif len(any_stop) &gt; 0:\nfirst_stop = any_stop[0]\ntext = all_text[: all_text.index(first_stop)]\nfinish_reason = \"stop\"\nbreak\nif stream:\nremaining_tokens = completion_tokens[returned_tokens:]\nremaining_text = self.detokenize(remaining_tokens)\nremaining_length = len(remaining_text)\n# We want to avoid yielding any characters from\n# the generated text if they are part of a stop\n# sequence.\nfirst_stop_position = 0\nfor s in stop_sequences:\nfor i in range(min(len(s), remaining_length), 0, -1):\nif remaining_text.endswith(s[:i]):\nif i &gt; first_stop_position:\nfirst_stop_position = i\nbreak\ntoken_end_position = 0\nfor token in remaining_tokens:\ntoken_end_position += len(self.detokenize([token]))\n# Check if stop sequence is in the token\nif token_end_position &gt;= (\nremaining_length - first_stop_position - 1\n):\nbreak\nlogprobs_or_none: Optional[CompletionLogprobs] = None\nif logprobs is not None:\ntoken_str = self.detokenize([token]).decode(\n\"utf-8\", errors=\"ignore\"\n)\ntext_offset = len(prompt) + len(\nself.detokenize(completion_tokens[:returned_tokens])\n)\ntoken_offset = len(prompt_tokens) + returned_tokens\nlogits = self._scores[token_offset - 1, :].tolist()\ncurrent_logprobs = Llama.logits_to_logprobs(logits)\nsorted_logprobs = list(\nsorted(\nzip(current_logprobs, range(len(current_logprobs))),\nreverse=True,\n)\n)\ntop_logprob = {\nself.detokenize([i]).decode(\n\"utf-8\", errors=\"ignore\"\n): logprob\nfor logprob, i in sorted_logprobs[:logprobs]\n}\ntop_logprob.update({token_str: current_logprobs[int(token)]})\nlogprobs_or_none = {\n\"tokens\": [\nself.detokenize([token]).decode(\n\"utf-8\", errors=\"ignore\"\n)\n],\n\"text_offset\": [text_offset],\n\"token_logprobs\": [sorted_logprobs[int(token)][0]],\n\"top_logprobs\": [top_logprob],\n}\nreturned_tokens += 1\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": self.detokenize([token]).decode(\n\"utf-8\", errors=\"ignore\"\n),\n\"index\": 0,\n\"logprobs\": logprobs_or_none,\n\"finish_reason\": None,\n}\n],\n}\nif len(completion_tokens) &gt;= max_tokens:\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"length\"\nbreak\nif stopping_criteria is not None and stopping_criteria(\nself._input_ids.tolist(), self._scores[-1, :].tolist()\n):\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"stop\"\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nif stream:\nremaining_tokens = completion_tokens[returned_tokens:]\nall_text = self.detokenize(remaining_tokens)\nany_stop = [s for s in stop_sequences if s in all_text]\nif len(any_stop) &gt; 0:\nend = min(all_text.index(stop) for stop in any_stop)\nelse:\nend = len(all_text)\ntoken_end_position = 0\nfor token in remaining_tokens:\ntoken_end_position += len(self.detokenize([token]))\nlogprobs_or_none: Optional[CompletionLogprobs] = None\nif logprobs is not None:\ntoken_str = self.detokenize([token]).decode(\n\"utf-8\", errors=\"ignore\"\n)\ntext_offset = len(prompt) + len(\nself.detokenize(completion_tokens[:returned_tokens])\n)\ntoken_offset = len(prompt_tokens) + returned_tokens - 1\nlogits = self._scores[token_offset, :].tolist()\ncurrent_logprobs = Llama.logits_to_logprobs(logits)\nsorted_logprobs = list(\nsorted(\nzip(current_logprobs, range(len(current_logprobs))),\nreverse=True,\n)\n)\ntop_logprob = {\nself.detokenize([i]).decode(\"utf-8\", errors=\"ignore\"): logprob\nfor logprob, i in sorted_logprobs[:logprobs]\n}\ntop_logprob.update({token_str: current_logprobs[int(token)]})\nlogprobs_or_none = {\n\"tokens\": [\nself.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n],\n\"text_offset\": [text_offset],\n\"token_logprobs\": [sorted_logprobs[int(token)][0]],\n\"top_logprobs\": [top_logprob],\n}\nif token_end_position &gt;= end:\nlast_text = self.detokenize([token])\nif token_end_position == end - 1:\nbreak\nreturned_tokens += 1\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": last_text[\n: len(last_text) - (token_end_position - end)\n].decode(\"utf-8\", errors=\"ignore\"),\n\"index\": 0,\n\"logprobs\": logprobs_or_none,\n\"finish_reason\": finish_reason,\n}\n],\n}\nbreak\nreturned_tokens += 1\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": self.detokenize([token]).decode(\n\"utf-8\", errors=\"ignore\"\n),\n\"index\": 0,\n\"logprobs\": logprobs_or_none,\n\"finish_reason\": finish_reason\nif returned_tokens == len(completion_tokens)\nelse None,\n}\n],\n}\nif self.cache:\nif self.verbose:\nprint(\"Llama._create_completion: cache save\", file=sys.stderr)\nself.cache[prompt_tokens + completion_tokens] = self.save_state()\nprint(\"Llama._create_completion: cache saved\", file=sys.stderr)\nreturn\nif self.cache:\nif self.verbose:\nprint(\"Llama._create_completion: cache save\", file=sys.stderr)\nself.cache[prompt_tokens + completion_tokens] = self.save_state()\ntext_str = text.decode(\"utf-8\", errors=\"ignore\")\nif echo:\ntext_str = prompt + text_str\nif suffix is not None:\ntext_str = text_str + suffix\nlogprobs_or_none: Optional[CompletionLogprobs] = None\nif logprobs is not None:\ntext_offset = 0 if echo else len(prompt)\ntoken_offset = 0 if echo else len(prompt_tokens[1:])\ntext_offsets: List[int] = []\ntoken_logprobs: List[Optional[float]] = []\ntokens: List[str] = []\ntop_logprobs: List[Optional[Dict[str, float]]] = []\nif echo:\n# Remove leading BOS token\nall_tokens = prompt_tokens[1:] + completion_tokens\nelse:\nall_tokens = completion_tokens\nall_token_strs = [\nself.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\nfor token in all_tokens\n]\nall_logprobs = [\nLlama.logits_to_logprobs(row.tolist()) for row in self._scores\n][token_offset:]\nfor token, token_str, logprobs_token in zip(\nall_tokens, all_token_strs, all_logprobs\n):\ntext_offsets.append(text_offset)\ntext_offset += len(token_str)\ntokens.append(token_str)\nsorted_logprobs = list(\nsorted(\nzip(logprobs_token, range(len(logprobs_token))), reverse=True\n)\n)\ntoken_logprobs.append(sorted_logprobs[int(token)][0])\ntop_logprob: Optional[Dict[str, float]] = {\nself.detokenize([i]).decode(\"utf-8\", errors=\"ignore\"): logprob\nfor logprob, i in sorted_logprobs[:logprobs]\n}\ntop_logprob.update({token_str: logprobs_token[int(token)]})\ntop_logprobs.append(top_logprob)\n# Weird idosincracy of the OpenAI API where\n# token_logprobs and top_logprobs are null for\n# the first token.\nif echo and len(all_tokens) &gt; 0:\ntoken_logprobs[0] = None\ntop_logprobs[0] = None\nlogprobs_or_none = {\n\"tokens\": tokens,\n\"text_offset\": text_offsets,\n\"token_logprobs\": token_logprobs,\n\"top_logprobs\": top_logprobs,\n}\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": text_str,\n\"index\": 0,\n\"logprobs\": logprobs_or_none,\n\"finish_reason\": finish_reason,\n}\n],\n\"usage\": {\n\"prompt_tokens\": len(prompt_tokens),\n\"completion_tokens\": len(completion_tokens),\n\"total_tokens\": len(prompt_tokens) + len(completion_tokens),\n},\n}\ndef create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\nstopping_criteria=stopping_criteria,\nlogits_processor=logits_processor,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\ndef __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\nstopping_criteria=stopping_criteria,\nlogits_processor=logits_processor,\n)\ndef _convert_text_completion_to_chat(\nself, completion: Completion\n) -&gt; ChatCompletion:\nreturn {\n\"id\": \"chat\" + completion[\"id\"],\n\"object\": \"chat.completion\",\n\"created\": completion[\"created\"],\n\"model\": completion[\"model\"],\n\"choices\": [\n{\n\"index\": 0,\n\"message\": {\n\"role\": \"assistant\",\n\"content\": completion[\"choices\"][0][\"text\"],\n},\n\"finish_reason\": completion[\"choices\"][0][\"finish_reason\"],\n}\n],\n\"usage\": completion[\"usage\"],\n}\ndef _convert_text_completion_chunks_to_chat(\nself,\nchunks: Iterator[CompletionChunk],\n) -&gt; Iterator[ChatCompletionChunk]:\nfor i, chunk in enumerate(chunks):\nif i == 0:\nyield {\n\"id\": \"chat\" + chunk[\"id\"],\n\"model\": chunk[\"model\"],\n\"created\": chunk[\"created\"],\n\"object\": \"chat.completion.chunk\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"role\": \"assistant\",\n},\n\"finish_reason\": None,\n}\n],\n}\nyield {\n\"id\": \"chat\" + chunk[\"id\"],\n\"model\": chunk[\"model\"],\n\"created\": chunk[\"created\"],\n\"object\": \"chat.completion.chunk\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"content\": chunk[\"choices\"][0][\"text\"],\n},\n\"finish_reason\": chunk[\"choices\"][0][\"finish_reason\"],\n}\n],\n}\ndef create_chat_completion(\nself,\nmessages: List[ChatCompletionMessage],\ntemperature: float = 0.2,\ntop_p: float = 0.95,\ntop_k: int = 40,\nstream: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nmax_tokens: int = 256,\npresence_penalty: float = 0.0,\nfrequency_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n\"\"\"Generate a chat completion from a list of messages.\n        Args:\n            messages: A list of messages to generate a response for.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n            stop: A list of strings to stop generation when encountered.\n            max_tokens: The maximum number of tokens to generate.\n            repeat_penalty: The penalty to apply to repeated tokens.\n        Returns:\n            Generated chat completion or a stream of chat completion chunks.\n        \"\"\"\nstop = (\nstop if isinstance(stop, list) else [stop] if isinstance(stop, str) else []\n)\nchat_history = \"\".join(\nf'### {\"Human\" if message[\"role\"] == \"user\" else \"Assistant\"}:{message[\"content\"]}'\nfor message in messages\n)\nPROMPT = chat_history + \"### Assistant:\"\nPROMPT_STOP = [\"### Assistant:\", \"### Human:\"]\ncompletion_or_chunks = self(\nprompt=PROMPT,\nstop=PROMPT_STOP + stop,\ntemperature=temperature,\ntop_p=top_p,\ntop_k=top_k,\nstream=stream,\nmax_tokens=max_tokens,\nrepeat_penalty=repeat_penalty,\npresence_penalty=presence_penalty,\nfrequency_penalty=frequency_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\nlogits_processor=logits_processor,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_chunks_to_chat(chunks)\nelse:\ncompletion: Completion = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_to_chat(completion)\ndef __del__(self):\nif self.ctx is not None:\nllama_cpp.llama_free(self.ctx)\nself.ctx = None\ndef __getstate__(self):\nreturn dict(\nverbose=self.verbose,\nmodel_path=self.model_path,\nn_ctx=self.params.n_ctx,\nn_gpu_layers=self.params.n_gpu_layers,\nseed=self.params.seed,\nf16_kv=self.params.f16_kv,\nlogits_all=self.params.logits_all,\nvocab_only=self.params.vocab_only,\nuse_mmap=self.params.use_mmap,\nuse_mlock=self.params.use_mlock,\nembedding=self.params.embedding,\nlow_vram=self.params.low_vram,\nlast_n_tokens_size=self.last_n_tokens_size,\nn_batch=self.n_batch,\nn_threads=self.n_threads,\nlora_base=self.lora_base,\nlora_path=self.lora_path,\n### DEPRECATED ###\nn_parts=self.n_parts,\n### DEPRECATED ###\n)\ndef __setstate__(self, state):\nself.__init__(\nmodel_path=state[\"model_path\"],\nn_ctx=state[\"n_ctx\"],\nn_parts=state[\"n_parts\"],\nn_gpu_layers=state[\"n_gpu_layers\"],\nseed=state[\"seed\"],\nf16_kv=state[\"f16_kv\"],\nlogits_all=state[\"logits_all\"],\nvocab_only=state[\"vocab_only\"],\nuse_mmap=state[\"use_mmap\"],\nuse_mlock=state[\"use_mlock\"],\nembedding=state[\"embedding\"],\nlow_vram=state[\"low_vram\"],\nn_threads=state[\"n_threads\"],\nn_batch=state[\"n_batch\"],\nlast_n_tokens_size=state[\"last_n_tokens_size\"],\nlora_base=state[\"lora_base\"],\nlora_path=state[\"lora_path\"],\nverbose=state[\"verbose\"],\n)\ndef save_state(self) -&gt; LlamaState:\nassert self.ctx is not None\nif self.verbose:\nprint(\"Llama.save_state: saving llama state\", file=sys.stderr)\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nif self.verbose:\nprint(f\"Llama.save_state: got state size: {state_size}\", file=sys.stderr)\nllama_state = (llama_cpp.c_uint8 * int(state_size))()\nif self.verbose:\nprint(\"Llama.save_state: allocated state\", file=sys.stderr)\nn_bytes = llama_cpp.llama_copy_state_data(self.ctx, llama_state)\nif self.verbose:\nprint(f\"Llama.save_state: copied llama state: {n_bytes}\", file=sys.stderr)\nif int(n_bytes) &gt; int(state_size):\nraise RuntimeError(\"Failed to copy llama state data\")\nllama_state_compact = (llama_cpp.c_uint8 * int(n_bytes))()\nllama_cpp.ctypes.memmove(llama_state_compact, llama_state, int(n_bytes))\nif self.verbose:\nprint(\nf\"Llama.save_state: saving {n_bytes} bytes of llama state\",\nfile=sys.stderr,\n)\nreturn LlamaState(\neval_tokens=self.eval_tokens.copy(),\neval_logits=self.eval_logits.copy(),\nscores=self._scores.copy(),\ninput_ids=self._input_ids.copy(),\nllama_state=llama_state_compact,\nllama_state_size=n_bytes,\n)\ndef load_state(self, state: LlamaState) -&gt; None:\nassert self.ctx is not None\nself.eval_tokens = state.eval_tokens.copy()\nself.eval_logits = state.eval_logits.copy()\nself._scores = state.scores.copy()\nself._input_ids = state.input_ids.copy()\nstate_size = state.llama_state_size\nif llama_cpp.llama_set_state_data(self.ctx, state.llama_state) != state_size:\nraise RuntimeError(\"Failed to set llama state data\")\ndef n_ctx(self) -&gt; int:\n\"\"\"Return the context window size.\"\"\"\nassert self.ctx is not None\nreturn llama_cpp.llama_n_ctx(self.ctx)\ndef n_embd(self) -&gt; int:\n\"\"\"Return the embedding size.\"\"\"\nassert self.ctx is not None\nreturn llama_cpp.llama_n_embd(self.ctx)\ndef n_vocab(self) -&gt; int:\n\"\"\"Return the vocabulary size.\"\"\"\nassert self.ctx is not None\nreturn llama_cpp.llama_n_vocab(self.ctx)\ndef tokenizer(self) -&gt; \"LlamaTokenizer\":\n\"\"\"Return the tokenizer for this model.\"\"\"\nassert self.ctx is not None\nreturn LlamaTokenizer(self)\n@staticmethod\ndef token_eos() -&gt; int:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n@staticmethod\ndef token_bos() -&gt; int:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n@staticmethod\ndef token_nl() -&gt; int:\n\"\"\"Return the newline token.\"\"\"\nreturn llama_cpp.llama_token_nl()\n@staticmethod\ndef logits_to_logprobs(logits: List[float]) -&gt; List[float]:\nexps = [math.exp(float(x)) for x in logits]\nsum_exps = sum(exps)\nreturn [math.log(x / sum_exps) for x in exps]\n@staticmethod\ndef longest_token_prefix(a: Sequence[int], b: Sequence[int]):\nlongest_prefix = 0\nfor _a, _b in zip(a, b):\nif _a == _b:\nlongest_prefix += 1\nelse:\nbreak\nreturn longest_prefix\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__init__","title":"<code>__init__(model_path, n_ctx=512, n_parts=-1, n_gpu_layers=0, seed=1337, f16_kv=True, logits_all=False, vocab_only=False, use_mmap=True, use_mlock=False, embedding=False, n_threads=None, n_batch=512, last_n_tokens_size=64, lora_base=None, lora_path=None, low_vram=False, verbose=True)</code>","text":"<p>Load a llama.cpp model from <code>model_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>n_ctx</code> <code>int</code> <p>Maximum context size.</p> <code>512</code> <code>n_parts</code> <code>int</code> <p>Number of parts to split the model into. If -1, the number of parts is automatically determined.</p> <code>-1</code> <code>seed</code> <code>int</code> <p>Random seed. 0 for random.</p> <code>1337</code> <code>f16_kv</code> <code>bool</code> <p>Use half-precision for key/value cache.</p> <code>True</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens, not just the last token.</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>Only load the vocabulary no weights.</p> <code>False</code> <code>use_mmap</code> <code>bool</code> <p>Use mmap if possible.</p> <code>True</code> <code>use_mlock</code> <code>bool</code> <p>Force the system to keep the model in RAM.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Embedding mode only.</p> <code>False</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads to use. If None, the number of threads is automatically determined.</p> <code>None</code> <code>n_batch</code> <code>int</code> <p>Maximum number of prompt tokens to batch together when calling llama_eval.</p> <code>512</code> <code>last_n_tokens_size</code> <code>int</code> <p>Maximum number of tokens to keep in the last_n_tokens deque.</p> <code>64</code> <code>lora_base</code> <code>Optional[str]</code> <p>Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.</p> <code>None</code> <code>lora_path</code> <code>Optional[str]</code> <p>Path to a LoRA file to apply to the model.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print verbose output to stderr.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model path does not exist.</p> <p>Returns:</p> Type Description <p>A Llama instance.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nn_gpu_layers: int = 0,\nseed: int = 1337,\nf16_kv: bool = True,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mmap: bool = True,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 512,\nlast_n_tokens_size: int = 64,\nlora_base: Optional[str] = None,\nlora_path: Optional[str] = None,\nlow_vram: bool = False,\nverbose: bool = True,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n    Args:\n        model_path: Path to the model.\n        n_ctx: Maximum context size.\n        n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n        seed: Random seed. 0 for random.\n        f16_kv: Use half-precision for key/value cache.\n        logits_all: Return logits for all tokens, not just the last token.\n        vocab_only: Only load the vocabulary no weights.\n        use_mmap: Use mmap if possible.\n        use_mlock: Force the system to keep the model in RAM.\n        embedding: Embedding mode only.\n        n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n        n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n        last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n        lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n        lora_path: Path to a LoRA file to apply to the model.\n        verbose: Print verbose output to stderr.\n    Raises:\n        ValueError: If the model path does not exist.\n    Returns:\n        A Llama instance.\n    \"\"\"\nself.verbose = verbose\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_gpu_layers = n_gpu_layers\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mmap = use_mmap if lora_path is None else False\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.params.low_vram = low_vram\nself.last_n_tokens_size = last_n_tokens_size\nself.n_batch = min(n_ctx, n_batch)\nself.eval_tokens: Deque[int] = deque(maxlen=n_ctx)\nself.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx if logits_all else 1)\nself.cache: Optional[BaseLlamaCache] = None\nself.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\nself.lora_base = lora_base\nself.lora_path = lora_path\n### DEPRECATED ###\nself.n_parts = n_parts\n### DEPRECATED ###\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\nassert self.ctx is not None\nif self.lora_path:\nif llama_cpp.llama_apply_lora_from_file(\nself.ctx,\nllama_cpp.c_char_p(self.lora_path.encode(\"utf-8\")),\nllama_cpp.c_char_p(self.lora_base.encode(\"utf-8\"))\nif self.lora_base is not None\nelse llama_cpp.c_char_p(0),\nllama_cpp.c_int(self.n_threads),\n):\nraise RuntimeError(\nf\"Failed to apply LoRA from lora path: {self.lora_path} to base path: {self.lora_base}\"\n)\nif self.verbose:\nprint(llama_cpp.llama_print_system_info().decode(\"utf-8\"), file=sys.stderr)\nself._n_vocab = self.n_vocab()\nself._n_ctx = self.n_ctx()\nsize = llama_cpp.c_size_t(self._n_vocab)\nsorted = llama_cpp.c_bool(False)\nself._candidates_data = np.array(\n[],\ndtype=np.dtype(\n[(\"id\", np.intc), (\"logit\", np.single), (\"p\", np.single)], align=True\n),\n)\nself._candidates_data.resize(3, self._n_vocab, refcheck=False)\ncandidates = llama_cpp.llama_token_data_array(\ndata=self._candidates_data.ctypes.data_as(llama_cpp.llama_token_data_p),\nsize=size,\nsorted=sorted,\n)\nself._candidates = candidates\nself._token_nl = Llama.token_nl()\nself._token_eos = Llama.token_eos()\nself._input_ids = np.array([], dtype=np.intc)\nself._scores = np.ndarray((0, self._n_vocab), dtype=np.single)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.tokenize","title":"<code>tokenize(text, add_bos=True)</code>","text":"<p>Tokenize a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>bytes</code> <p>The utf-8 encoded string to tokenize.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the tokenization failed.</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>A list of tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def tokenize(self, text: bytes, add_bos: bool = True) -&gt; List[int]:\n\"\"\"Tokenize a string.\n    Args:\n        text: The utf-8 encoded string to tokenize.\n    Raises:\n        RuntimeError: If the tokenization failed.\n    Returns:\n        A list of tokens.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = self._n_ctx\ntokens = (llama_cpp.llama_token * n_ctx)()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nllama_cpp.c_int(n_ctx),\nllama_cpp.c_bool(add_bos),\n)\nif n_tokens &lt; 0:\nn_tokens = abs(n_tokens)\ntokens = (llama_cpp.llama_token * n_tokens)()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nllama_cpp.c_int(n_tokens),\nllama_cpp.c_bool(add_bos),\n)\nif n_tokens &lt; 0:\nraise RuntimeError(\nf'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}'\n)\nreturn list(tokens[:n_tokens])\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.detokenize","title":"<code>detokenize(tokens)</code>","text":"<p>Detokenize a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[int]</code> <p>The list of tokens to detokenize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The detokenized string.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def detokenize(self, tokens: List[int]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n    Args:\n        tokens: The list of tokens to detokenize.\n    Returns:\n        The detokenized string.\n    \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(\nself.ctx, llama_cpp.llama_token(token)\n)\nreturn output\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.reset","title":"<code>reset()</code>","text":"<p>Reset the model state.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.eval_tokens.clear()\nself.eval_logits.clear()\nself._input_ids = np.array([], dtype=np.intc)\nself._scores = np.ndarray((0, self._n_vocab), dtype=np.single)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.eval","title":"<code>eval(tokens)</code>","text":"<p>Evaluate a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[int]</code> <p>The list of tokens to evaluate.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def eval(self, tokens: Sequence[int]):\n\"\"\"Evaluate a list of tokens.\n    Args:\n        tokens: The list of tokens to evaluate.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = self._n_ctx\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), len(self._input_ids))\nn_tokens = len(batch)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(n_tokens),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif return_code != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\n# Save tokens\nself.eval_tokens.extend(batch)\nself._input_ids: npt.NDArray[np.intc] = np.concatenate(\n(self._input_ids, np.array(batch, dtype=np.intc)), axis=0\n)\n# Save logits\nrows = n_tokens if self.params.logits_all else 1\nn_vocab = self._n_vocab\ncols = n_vocab\nlogits_view = llama_cpp.llama_get_logits(self.ctx)\nlogits = [logits_view[i * cols : (i + 1) * cols] for i in range(rows)]\nself.eval_logits.extend(logits)\nself._scores: npt.NDArray[np.single] = np.concatenate(\n(self._scores, np.array(logits, dtype=np.single)), axis=0\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.sample","title":"<code>sample(top_k=40, top_p=0.95, temp=0.8, repeat_penalty=1.1, frequency_penalty=0.0, presence_penalty=0.0, tfs_z=1.0, mirostat_mode=0, mirostat_eta=0.1, mirostat_tau=5.0, penalize_nl=True, logits_processor=None)</code>","text":"<p>Sample a token from the model.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> <code>40</code> <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> <code>0.95</code> <code>temp</code> <code>float</code> <p>The temperature parameter.</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> <code>1.1</code> <p>Returns:</p> Type Description <p>The sampled token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def sample(\nself,\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_eta: float = 0.1,\nmirostat_tau: float = 5.0,\npenalize_nl: bool = True,\nlogits_processor: Optional[LogitsProcessorList] = None,\n):\n\"\"\"Sample a token from the model.\n    Args:\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n    Returns:\n        The sampled token.\n    \"\"\"\nassert self.ctx is not None\nlast_n_tokens_data = [llama_cpp.llama_token(0)] * max(\n0, self.last_n_tokens_size - len(self._input_ids)\n) + self._input_ids[-self.last_n_tokens_size :].tolist()\nreturn self._sample(\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\ntfs_z=llama_cpp.c_float(tfs_z),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\nfrequency_penalty=llama_cpp.c_float(frequency_penalty),\npresence_penalty=llama_cpp.c_float(presence_penalty),\nmirostat_mode=llama_cpp.c_int(mirostat_mode),\nmirostat_tau=llama_cpp.c_float(mirostat_tau),\nmirostat_eta=llama_cpp.c_float(mirostat_eta),\npenalize_nl=penalize_nl,\nlogits_processor=logits_processor,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.generate","title":"<code>generate(tokens, top_k=40, top_p=0.95, temp=0.8, repeat_penalty=1.1, reset=True, frequency_penalty=0.0, presence_penalty=0.0, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, logits_processor=None, stopping_criteria=None)</code>","text":"<p>Create a generator of tokens from a prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n&gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n&gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n...     print(llama.detokenize([token]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[int]</code> <p>The prompt tokens.</p> required <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> <code>40</code> <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> <code>0.95</code> <code>temp</code> <code>float</code> <p>The temperature parameter.</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> <code>1.1</code> <code>reset</code> <code>bool</code> <p>Whether to reset the model state.</p> <code>True</code> <p>Yields:</p> Type Description <code>Generator[int, Optional[Sequence[int]], None]</code> <p>The generated tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def generate(\nself,\ntokens: Sequence[int],\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nreset: bool = True,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nlogits_processor: Optional[LogitsProcessorList] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\n) -&gt; Generator[int, Optional[Sequence[int]], None]:\n\"\"\"Create a generator of tokens from a prompt.\n    Examples:\n        &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n        &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n        &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n        ...     print(llama.detokenize([token]))\n    Args:\n        tokens: The prompt tokens.\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n        reset: Whether to reset the model state.\n    Yields:\n        The generated tokens.\n    \"\"\"\nassert self.ctx is not None\nif reset and len(self._input_ids) &gt; 0:\nlongest_prefix = 0\nfor a, b in zip(self._input_ids, tokens[:-1]):\nif a == b:\nlongest_prefix += 1\nelse:\nbreak\nif longest_prefix &gt; 0:\nif self.verbose:\nprint(\"Llama.generate: prefix-match hit\", file=sys.stderr)\nreset = False\ntokens = tokens[longest_prefix:]\nself._input_ids = self._input_ids[:longest_prefix]\nself._scores = self._scores[:longest_prefix, :]\nfor _ in range(len(self.eval_tokens) - longest_prefix):\nself.eval_tokens.pop()\ntry:\nself.eval_logits.pop()\nexcept IndexError:\npass\nif reset:\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nlogits_processor=logits_processor,\n)\nif stopping_criteria is not None and stopping_criteria(\nself._input_ids.tolist(), self._scores[-1, :].tolist()\n):\nreturn\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_embedding","title":"<code>create_embedding(input, model=None)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[str, List[str]]</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>An embedding object.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_embedding(\nself, input: Union[str, List[str]], model: Optional[str] = None\n) -&gt; Embedding:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        An embedding object.\n    \"\"\"\nassert self.ctx is not None\nmodel_name: str = model if model is not None else self.model_path\nif self.params.embedding == False:\nraise RuntimeError(\n\"Llama model must be created with embedding=True to call this method\"\n)\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\nif isinstance(input, str):\ninputs = [input]\nelse:\ninputs = input\ndata: List[EmbeddingData] = []\ntotal_tokens = 0\nfor index, input in enumerate(inputs):\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\ntotal_tokens += n_tokens\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\ndata.append(\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": index,\n}\n)\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nreturn {\n\"object\": \"list\",\n\"data\": data,\n\"model\": model_name,\n\"usage\": {\n\"prompt_tokens\": total_tokens,\n\"total_tokens\": total_tokens,\n},\n}\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.embed","title":"<code>embed(input)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>A list of embeddings</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def embed(self, input: str) -&gt; List[float]:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        A list of embeddings\n    \"\"\"\nreturn list(map(float, self.create_embedding(input)[\"data\"][0][\"embedding\"]))\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_completion","title":"<code>create_completion(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], frequency_penalty=0.0, presence_penalty=0.0, repeat_penalty=1.1, top_k=40, stream=False, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None, stopping_criteria=None, logits_processor=None)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Iterator[CompletionChunk]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\nstopping_criteria=stopping_criteria,\nlogits_processor=logits_processor,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__call__","title":"<code>__call__(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], frequency_penalty=0.0, presence_penalty=0.0, repeat_penalty=1.1, top_k=40, stream=False, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None, stopping_criteria=None, logits_processor=None)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Iterator[CompletionChunk]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nstopping_criteria: Optional[StoppingCriteriaList] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\nstopping_criteria=stopping_criteria,\nlogits_processor=logits_processor,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_chat_completion","title":"<code>create_chat_completion(messages, temperature=0.2, top_p=0.95, top_k=40, stream=False, stop=[], max_tokens=256, presence_penalty=0.0, frequency_penalty=0.0, repeat_penalty=1.1, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None, logits_processor=None)</code>","text":"<p>Generate a chat completion from a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatCompletionMessage]</code> <p>A list of messages to generate a response for.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.2</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <code>stop</code> <code>Optional[Union[str, List[str]]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>256</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>Union[ChatCompletion, Iterator[ChatCompletionChunk]]</code> <p>Generated chat completion or a stream of chat completion chunks.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_chat_completion(\nself,\nmessages: List[ChatCompletionMessage],\ntemperature: float = 0.2,\ntop_p: float = 0.95,\ntop_k: int = 40,\nstream: bool = False,\nstop: Optional[Union[str, List[str]]] = [],\nmax_tokens: int = 256,\npresence_penalty: float = 0.0,\nfrequency_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\nlogits_processor: Optional[LogitsProcessorList] = None,\n) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n\"\"\"Generate a chat completion from a list of messages.\n    Args:\n        messages: A list of messages to generate a response for.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n        stop: A list of strings to stop generation when encountered.\n        max_tokens: The maximum number of tokens to generate.\n        repeat_penalty: The penalty to apply to repeated tokens.\n    Returns:\n        Generated chat completion or a stream of chat completion chunks.\n    \"\"\"\nstop = (\nstop if isinstance(stop, list) else [stop] if isinstance(stop, str) else []\n)\nchat_history = \"\".join(\nf'### {\"Human\" if message[\"role\"] == \"user\" else \"Assistant\"}:{message[\"content\"]}'\nfor message in messages\n)\nPROMPT = chat_history + \"### Assistant:\"\nPROMPT_STOP = [\"### Assistant:\", \"### Human:\"]\ncompletion_or_chunks = self(\nprompt=PROMPT,\nstop=PROMPT_STOP + stop,\ntemperature=temperature,\ntop_p=top_p,\ntop_k=top_k,\nstream=stream,\nmax_tokens=max_tokens,\nrepeat_penalty=repeat_penalty,\npresence_penalty=presence_penalty,\nfrequency_penalty=frequency_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\nlogits_processor=logits_processor,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_chunks_to_chat(chunks)\nelse:\ncompletion: Completion = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_to_chat(completion)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.set_cache","title":"<code>set_cache(cache)</code>","text":"<p>Set the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>Optional[BaseLlamaCache]</code> <p>The cache to set.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def set_cache(self, cache: Optional[BaseLlamaCache]):\n\"\"\"Set the cache.\n    Args:\n        cache: The cache to set.\n    \"\"\"\nself.cache = cache\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.save_state","title":"<code>save_state()</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>def save_state(self) -&gt; LlamaState:\nassert self.ctx is not None\nif self.verbose:\nprint(\"Llama.save_state: saving llama state\", file=sys.stderr)\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nif self.verbose:\nprint(f\"Llama.save_state: got state size: {state_size}\", file=sys.stderr)\nllama_state = (llama_cpp.c_uint8 * int(state_size))()\nif self.verbose:\nprint(\"Llama.save_state: allocated state\", file=sys.stderr)\nn_bytes = llama_cpp.llama_copy_state_data(self.ctx, llama_state)\nif self.verbose:\nprint(f\"Llama.save_state: copied llama state: {n_bytes}\", file=sys.stderr)\nif int(n_bytes) &gt; int(state_size):\nraise RuntimeError(\"Failed to copy llama state data\")\nllama_state_compact = (llama_cpp.c_uint8 * int(n_bytes))()\nllama_cpp.ctypes.memmove(llama_state_compact, llama_state, int(n_bytes))\nif self.verbose:\nprint(\nf\"Llama.save_state: saving {n_bytes} bytes of llama state\",\nfile=sys.stderr,\n)\nreturn LlamaState(\neval_tokens=self.eval_tokens.copy(),\neval_logits=self.eval_logits.copy(),\nscores=self._scores.copy(),\ninput_ids=self._input_ids.copy(),\nllama_state=llama_state_compact,\nllama_state_size=n_bytes,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.load_state","title":"<code>load_state(state)</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>def load_state(self, state: LlamaState) -&gt; None:\nassert self.ctx is not None\nself.eval_tokens = state.eval_tokens.copy()\nself.eval_logits = state.eval_logits.copy()\nself._scores = state.scores.copy()\nself._input_ids = state.input_ids.copy()\nstate_size = state.llama_state_size\nif llama_cpp.llama_set_state_data(self.ctx, state.llama_state) != state_size:\nraise RuntimeError(\"Failed to set llama state data\")\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_bos","title":"<code>token_bos()</code>  <code>staticmethod</code>","text":"<p>Return the beginning-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_bos() -&gt; int:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_eos","title":"<code>token_eos()</code>  <code>staticmethod</code>","text":"<p>Return the end-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_eos() -&gt; int:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.LlamaCache","title":"<code>llama_cpp.LlamaCache = LlamaRAMCache</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.LlamaState","title":"<code>llama_cpp.LlamaState</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>class LlamaState:\ndef __init__(\nself,\neval_tokens: Deque[int],\neval_logits: Deque[List[float]],\ninput_ids: npt.NDArray[np.intc],\nscores: npt.NDArray[np.single],\nllama_state,  # type: llama_cpp.Array[llama_cpp.c_uint8]\nllama_state_size: int,\n):\nself.eval_tokens = eval_tokens\nself.eval_logits = eval_logits\nself.input_ids = input_ids\nself.scores = scores\nself.llama_state = llama_state\nself.llama_state_size = llama_state_size\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.GGML_CUDA_MAX_DEVICES","title":"<code>GGML_CUDA_MAX_DEVICES = ctypes.c_int(16)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.GGML_USE_CUBLAS","title":"<code>GGML_USE_CUBLAS = hasattr(_lib, 'ggml_init_cublas')</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC","title":"<code>LLAMA_FILE_MAGIC = LLAMA_FILE_MAGIC_GGJT</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_GGJT","title":"<code>LLAMA_FILE_MAGIC_GGJT = ctypes.c_uint(1734830708)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_GGLA","title":"<code>LLAMA_FILE_MAGIC_GGLA = ctypes.c_uint(1734831201)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_GGMF","title":"<code>LLAMA_FILE_MAGIC_GGMF = ctypes.c_uint(1734831462)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_GGML","title":"<code>LLAMA_FILE_MAGIC_GGML = ctypes.c_uint(1734831468)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_GGSN","title":"<code>LLAMA_FILE_MAGIC_GGSN = ctypes.c_uint(1734833006)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_UNVERSIONED","title":"<code>LLAMA_FILE_MAGIC_UNVERSIONED = LLAMA_FILE_MAGIC_GGML</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_VERSION","title":"<code>LLAMA_FILE_VERSION = c_int(3)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_ALL_F32","title":"<code>LLAMA_FTYPE_ALL_F32 = c_int(0)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_F16","title":"<code>LLAMA_FTYPE_MOSTLY_F16 = c_int(1)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q2_K","title":"<code>LLAMA_FTYPE_MOSTLY_Q2_K = c_int(10)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q3_K_L","title":"<code>LLAMA_FTYPE_MOSTLY_Q3_K_L = c_int(13)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q3_K_M","title":"<code>LLAMA_FTYPE_MOSTLY_Q3_K_M = c_int(12)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q3_K_S","title":"<code>LLAMA_FTYPE_MOSTLY_Q3_K_S = c_int(11)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_0 = c_int(2)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_1","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_1 = c_int(3)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = c_int(4)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_K_M","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_K_M = c_int(15)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_K_S","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_K_S = c_int(14)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q5_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q5_0 = c_int(8)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q5_1","title":"<code>LLAMA_FTYPE_MOSTLY_Q5_1 = c_int(9)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q5_K_M","title":"<code>LLAMA_FTYPE_MOSTLY_Q5_K_M = c_int(17)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q5_K_S","title":"<code>LLAMA_FTYPE_MOSTLY_Q5_K_S = c_int(16)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q6_K","title":"<code>LLAMA_FTYPE_MOSTLY_Q6_K = c_int(18)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q8_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q8_0 = c_int(7)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_MAX_DEVICES","title":"<code>LLAMA_MAX_DEVICES = GGML_CUDA_MAX_DEVICES if GGML_USE_CUBLAS else ctypes.c_int(1)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_SESSION_MAGIC","title":"<code>LLAMA_SESSION_MAGIC = LLAMA_FILE_MAGIC_GGSN</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_SESSION_VERSION","title":"<code>LLAMA_SESSION_VERSION = c_int(1)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.c_float_p","title":"<code>c_float_p = POINTER(c_float)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.c_size_t_p","title":"<code>c_size_t_p = POINTER(c_size_t)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.c_uint8_p","title":"<code>c_uint8_p = POINTER(c_uint8)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_p","title":"<code>llama_context_p = c_void_p</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params_p","title":"<code>llama_context_params_p = POINTER(llama_context_params)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_progress_callback","title":"<code>llama_progress_callback = ctypes.CFUNCTYPE(None, c_float, c_void_p)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token","title":"<code>llama_token = c_int</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_data_array_p","title":"<code>llama_token_data_array_p = POINTER(llama_token_data_array)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_data_p","title":"<code>llama_token_data_p = POINTER(llama_token_data)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_p","title":"<code>llama_token_p = POINTER(llama_token)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params","title":"<code>llama_context_params</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_context_params(Structure):\n_fields_ = [\n(\"n_ctx\", c_int),\n(\"n_batch\", c_int),\n(\"n_gpu_layers\", c_int),\n(\"main_gpu\", c_int),\n(\"tensor_split\", c_float * LLAMA_MAX_DEVICES.value),\n(\"low_vram\", c_bool),\n(\"seed\", c_int),\n(\"f16_kv\", c_bool),\n(\n\"logits_all\",\nc_bool,\n),\n(\"vocab_only\", c_bool),\n(\"use_mmap\", c_bool),\n(\"use_mlock\", c_bool),\n(\"embedding\", c_bool),\n(\"progress_callback\", llama_progress_callback),\n(\"progress_callback_user_data\", c_void_p),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_model_quantize_params","title":"<code>llama_model_quantize_params</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_model_quantize_params(Structure):\n_fields_ = [\n(\"nthread\", c_int),\n(\"ftype\", c_int),\n(\"allow_requantize\", c_bool),\n(\"quantize_output_tensor\", c_bool),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_data","title":"<code>llama_token_data</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_token_data(Structure):\n_fields_ = [\n(\"id\", llama_token),\n(\"logit\", c_float),\n(\"p\", c_float),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_data_array","title":"<code>llama_token_data_array</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_token_data_array(Structure):\n_fields_ = [\n(\"data\", llama_token_data_p),\n(\"size\", c_size_t),\n(\"sorted\", c_bool),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_apply_lora_from_file","title":"<code>llama_apply_lora_from_file(ctx, path_lora, path_base_model, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_apply_lora_from_file(\nctx: llama_context_p,\npath_lora: c_char_p,\npath_base_model: c_char_p,\nn_threads: c_int,\n) -&gt; int:\nreturn _lib.llama_apply_lora_from_file(ctx, path_lora, path_base_model, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_context_default_params","title":"<code>llama_context_default_params()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_context_default_params() -&gt; llama_context_params:\nreturn _lib.llama_context_default_params()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_copy_state_data","title":"<code>llama_copy_state_data(ctx, dst)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_copy_state_data(\nctx: llama_context_p, dst  # type: Array[c_uint8]\n) -&gt; int:\nreturn _lib.llama_copy_state_data(ctx, dst)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_eval","title":"<code>llama_eval(ctx, tokens, n_tokens, n_past, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_eval(\nctx: llama_context_p,\ntokens,  # type: Array[llama_token]\nn_tokens: c_int,\nn_past: c_int,\nn_threads: c_int,\n) -&gt; int:\nreturn _lib.llama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_free","title":"<code>llama_free(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_free(ctx: llama_context_p):\nreturn _lib.llama_free(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_embeddings","title":"<code>llama_get_embeddings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_embeddings(\nctx: llama_context_p,\n):  # type: (...) -&gt; Array[float] # type: ignore\nreturn _lib.llama_get_embeddings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_kv_cache_token_count","title":"<code>llama_get_kv_cache_token_count(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_kv_cache_token_count(ctx: llama_context_p) -&gt; int:\nreturn _lib.llama_get_kv_cache_token_count(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_logits","title":"<code>llama_get_logits(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_logits(\nctx: llama_context_p,\n):  # type: (...) -&gt; Array[float] # type: ignore\nreturn _lib.llama_get_logits(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_state_size","title":"<code>llama_get_state_size(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_state_size(ctx: llama_context_p) -&gt; int:\nreturn _lib.llama_get_state_size(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_vocab","title":"<code>llama_get_vocab(ctx, strings, scores, capacity)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_vocab(\nctx: llama_context_p,\nstrings,  # type: Array[c_char_p] # type: ignore\nscores,  # type: Array[c_float] # type: ignore\ncapacity: c_int,\n) -&gt; int:\nreturn _lib.llama_get_vocab(ctx, strings, scores, capacity)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_init_backend","title":"<code>llama_init_backend()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_init_backend():\nreturn _lib.llama_init_backend()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_init_from_file","title":"<code>llama_init_from_file(path_model, params)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_init_from_file(\npath_model: bytes, params: llama_context_params\n) -&gt; llama_context_p:\nreturn _lib.llama_init_from_file(path_model, params)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_load_session_file","title":"<code>llama_load_session_file(ctx, path_session, tokens_out, n_token_capacity, n_token_count_out)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_load_session_file(\nctx: llama_context_p,\npath_session: bytes,\ntokens_out,  # type: Array[llama_token]\nn_token_capacity: c_size_t,\nn_token_count_out,  # type: _Pointer[c_size_t]\n) -&gt; int:\nreturn _lib.llama_load_session_file(\nctx, path_session, tokens_out, n_token_capacity, n_token_count_out\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_mlock_supported","title":"<code>llama_mlock_supported()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_mlock_supported() -&gt; bool:\nreturn _lib.llama_mlock_supported()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_mmap_supported","title":"<code>llama_mmap_supported()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_mmap_supported() -&gt; bool:\nreturn _lib.llama_mmap_supported()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_model_quantize","title":"<code>llama_model_quantize(fname_inp, fname_out, params)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_model_quantize(\nfname_inp: bytes,\nfname_out: bytes,\nparams,  # type: POINTER(llama_model_quantize_params) # type: ignore\n) -&gt; int:\nreturn _lib.llama_model_quantize(fname_inp, fname_out, params)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_model_quantize_default_params","title":"<code>llama_model_quantize_default_params()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_model_quantize_default_params() -&gt; llama_model_quantize_params:\nreturn _lib.llama_model_quantize_default_params()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_ctx","title":"<code>llama_n_ctx(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_ctx(ctx: llama_context_p) -&gt; int:\nreturn _lib.llama_n_ctx(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_embd","title":"<code>llama_n_embd(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_embd(ctx: llama_context_p) -&gt; int:\nreturn _lib.llama_n_embd(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_vocab","title":"<code>llama_n_vocab(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_vocab(ctx: llama_context_p) -&gt; int:\nreturn _lib.llama_n_vocab(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_system_info","title":"<code>llama_print_system_info()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_system_info() -&gt; bytes:\nreturn _lib.llama_print_system_info()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_timings","title":"<code>llama_print_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_timings(ctx: llama_context_p):\n_lib.llama_print_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_reset_timings","title":"<code>llama_reset_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_reset_timings(ctx: llama_context_p):\n_lib.llama_reset_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_frequency_and_presence_penalties","title":"<code>llama_sample_frequency_and_presence_penalties(ctx, candidates, last_tokens_data, last_tokens_size, alpha_frequency, alpha_presence)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_frequency_and_presence_penalties(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nlast_tokens_data,  # type: Array[llama_token]\nlast_tokens_size: c_int,\nalpha_frequency: c_float,\nalpha_presence: c_float,\n):\nreturn _lib.llama_sample_frequency_and_presence_penalties(\nctx,\ncandidates,\nlast_tokens_data,\nlast_tokens_size,\nalpha_frequency,\nalpha_presence,\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_repetition_penalty","title":"<code>llama_sample_repetition_penalty(ctx, candidates, last_tokens_data, last_tokens_size, penalty)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_repetition_penalty(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nlast_tokens_data,  # type: Array[llama_token]\nlast_tokens_size: c_int,\npenalty: c_float,\n):\nreturn _lib.llama_sample_repetition_penalty(\nctx, candidates, last_tokens_data, last_tokens_size, penalty\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_softmax","title":"<code>llama_sample_softmax(ctx, candidates)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_softmax(\nctx: llama_context_p, candidates  # type: _Pointer[llama_token_data]\n):\nreturn _lib.llama_sample_softmax(ctx, candidates)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_tail_free","title":"<code>llama_sample_tail_free(ctx, candidates, z, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_tail_free(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nz: c_float,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_tail_free(ctx, candidates, z, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_temperature","title":"<code>llama_sample_temperature(ctx, candidates, temp)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_temperature(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\ntemp: c_float,\n):\nreturn _lib.llama_sample_temperature(ctx, candidates, temp)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token","title":"<code>llama_sample_token(ctx, candidates)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\n) -&gt; int:\nreturn _lib.llama_sample_token(ctx, candidates)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token_greedy","title":"<code>llama_sample_token_greedy(ctx, candidates)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token_greedy(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\n) -&gt; int:\nreturn _lib.llama_sample_token_greedy(ctx, candidates)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token_mirostat","title":"<code>llama_sample_token_mirostat(ctx, candidates, tau, eta, m, mu)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token_mirostat(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\ntau: c_float,\neta: c_float,\nm: c_int,\nmu,  # type: _Pointer[c_float]\n) -&gt; int:\nreturn _lib.llama_sample_token_mirostat(ctx, candidates, tau, eta, m, mu)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token_mirostat_v2","title":"<code>llama_sample_token_mirostat_v2(ctx, candidates, tau, eta, mu)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token_mirostat_v2(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\ntau: c_float,\neta: c_float,\nmu,  # type: _Pointer[c_float]\n) -&gt; int:\nreturn _lib.llama_sample_token_mirostat_v2(ctx, candidates, tau, eta, mu)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_top_k","title":"<code>llama_sample_top_k(ctx, candidates, k, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_top_k(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nk: c_int,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_top_k(ctx, candidates, k, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_top_p","title":"<code>llama_sample_top_p(ctx, candidates, p, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_top_p(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\np: c_float,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_top_p(ctx, candidates, p, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_typical","title":"<code>llama_sample_typical(ctx, candidates, p, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_typical(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\np: c_float,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_typical(ctx, candidates, p, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_save_session_file","title":"<code>llama_save_session_file(ctx, path_session, tokens, n_token_count)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_save_session_file(\nctx: llama_context_p,\npath_session: bytes,\ntokens,  # type: Array[llama_token]\nn_token_count: c_size_t,\n) -&gt; int:\nreturn _lib.llama_save_session_file(ctx, path_session, tokens, n_token_count)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_rng_seed","title":"<code>llama_set_rng_seed(ctx, seed)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_rng_seed(ctx: llama_context_p, seed: c_int):\nreturn _lib.llama_set_rng_seed(ctx, seed)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_state_data","title":"<code>llama_set_state_data(ctx, src)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_state_data(\nctx: llama_context_p, src  # type: Array[c_uint8]\n) -&gt; int:\nreturn _lib.llama_set_state_data(ctx, src)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_time_us","title":"<code>llama_time_us()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_time_us() -&gt; int:\nreturn _lib.llama_time_us()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_bos","title":"<code>llama_token_bos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_bos() -&gt; int:\nreturn _lib.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_eos","title":"<code>llama_token_eos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_eos() -&gt; int:\nreturn _lib.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_nl","title":"<code>llama_token_nl()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_nl() -&gt; int:\nreturn _lib.llama_token_nl()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_to_str","title":"<code>llama_token_to_str(ctx, token)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_to_str(ctx: llama_context_p, token: llama_token) -&gt; bytes:\nreturn _lib.llama_token_to_str(ctx, token)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_tokenize","title":"<code>llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_tokenize(\nctx: llama_context_p,\ntext: bytes,\ntokens,  # type: Array[llama_token]\nn_max_tokens: c_int,\nadd_bos: c_bool,\n) -&gt; int:\nreturn _lib.llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"},{"location":"macos_install/","title":"llama-cpp-python - MacOS Install with Metal GPU","text":"<p>(1) Make sure you have xcode installed... at least the command line parts <pre><code># check the path of your xcode install \nxcode-select -p\n\n# xcode installed returns\n# /Applications/Xcode-beta.app/Contents/Developer\n\n# if xcode is missing then install it... it takes ages;\nxcode-select --install\n</code></pre></p> <p>(2) Install the conda version for MacOS that supports Metal GPU <pre><code>wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\nbash Miniforge3-MacOSX-arm64.sh\n</code></pre></p> <p>(3) Make a conda environment <pre><code>conda create -n llama python=3.9.16\nconda activate llama\n</code></pre></p> <p>(4) Install the LATEST llama-cpp-python.. which, as of just today, happily supports MacOS Metal GPU (you needed xcode installed in order pip to build/compile the C++ code) <pre><code>pip uninstall llama-cpp-python -y\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir\npip install 'llama-cpp-python[server]'\n\n# you should now have llama-cpp-python v0.1.62 installed\nllama-cpp-python \u00a0 \u00a0 \u00a0 \u00a0 0.1.62\u00a0 \u00a0 \u00a0 \n</code></pre></p> <p>(4) Download a v3 ggml llama/vicuna/alpaca model  - ggmlv3  - file name ends with q4_0.bin - indicating it is 4bit quantized, with quantisation method 0</p> <p>https://huggingface.co/vicuna/ggml-vicuna-13b-1.1/blob/main/ggml-vic13b-q4_0.bin https://huggingface.co/vicuna/ggml-vicuna-13b-1.1/blob/main/ggml-vic13b-uncensored-q4_0.bin https://huggingface.co/TheBloke/LLaMa-7B-GGML/blob/main/llama-7b.ggmlv3.q4_0.bin https://huggingface.co/TheBloke/LLaMa-13B-GGML/blob/main/llama-13b.ggmlv3.q4_0.bin</p> <p>(6) run the llama-cpp-python API server with MacOS Metal GPU support <pre><code># config your ggml model path\n# make sure it is ggml v3\n# make sure it is q4_0\nexport MODEL=[path to your llama.cpp ggml models]]/[ggml-model-name]]q4_0.bin\npython3 -m llama_cpp.server --model $MODEL  --n_gpu_layers 1\n</code></pre></p> <p>Note: If you omit the <code>--n_gpu_layers 1</code> then CPU will be used</p>"}]}