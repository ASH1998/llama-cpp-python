{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#python-bindings-for-llamacpp","title":"\ud83e\udd99 Python Bindings for <code>llama.cpp</code>","text":"<p>Simple Python bindings for @ggerganov's <code>llama.cpp</code> library. This package provides:</p> <ul> <li>Low-level access to C API via <code>ctypes</code> interface.</li> <li>High-level Python API for text completion</li> <li>OpenAI-like API</li> <li>LangChain compatibility</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install llama-cpp-python\n</code></pre>"},{"location":"#high-level-api","title":"High-level API","text":"<pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; llm = Llama(model_path=\"./models/7B/ggml-model.bin\")\n&gt;&gt;&gt; output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n&gt;&gt;&gt; print(output)\n{\n\"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\"object\": \"text_completion\",\n\"created\": 1679561337,\n\"model\": \"./models/7B/ggml-model.bin\",\n\"choices\": [\n{\n\"text\": \"Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\",\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": \"stop\"\n}\n],\n\"usage\": {\n\"prompt_tokens\": 14,\n\"completion_tokens\": 28,\n\"total_tokens\": 42\n}\n}\n</code></pre>"},{"location":"#web-server","title":"Web Server","text":"<p><code>llama-cpp-python</code> offers a web server which aims to act as a drop-in replacement for the OpenAI API. This allows you to use llama.cpp compatible models with any OpenAI compatible client (language libraries, services, etc).</p> <p>To install the server package and get started:</p> <pre><code>pip install llama-cpp-python[server]\nexport MODEL=./models/7B/ggml-model.bin\npython3 -m llama_cpp.server\n</code></pre> <p>Navigate to http://localhost:8000/docs to see the OpenAPI documentation.</p>"},{"location":"#low-level-api","title":"Low-level API","text":"<p>The low-level API is a direct <code>ctypes</code> binding to the C API provided by <code>llama.cpp</code>. The entire API can be found in llama_cpp/llama_cpp.py and should mirror llama.h.</p>"},{"location":"#development","title":"Development","text":"<p>This package is under active development and I welcome any contributions.</p> <p>To get started, clone the repository and install the package in development mode:</p> <pre><code>git clone git@github.com:abetlen/llama-cpp-python.git\ngit submodule update --init --recursive\n# Will need to be re-run any time vendor/llama.cpp is updated\npython3 setup.py develop\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":""},{"location":"#llama_cpp.Llama","title":"<code>llama_cpp.Llama</code>","text":"<p>High-level Python wrapper for a llama.cpp model.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>class Llama:\n\"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\ndef __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nn_gpu_layers: int = 0,\nseed: int = 1337,\nf16_kv: bool = True,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mmap: bool = True,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 512,\nlast_n_tokens_size: int = 64,\nlora_base: Optional[str] = None,\nlora_path: Optional[str] = None,\nverbose: bool = True,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n        Args:\n            model_path: Path to the model.\n            n_ctx: Maximum context size.\n            n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n            seed: Random seed. 0 for random.\n            f16_kv: Use half-precision for key/value cache.\n            logits_all: Return logits for all tokens, not just the last token.\n            vocab_only: Only load the vocabulary no weights.\n            use_mmap: Use mmap if possible.\n            use_mlock: Force the system to keep the model in RAM.\n            embedding: Embedding mode only.\n            n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n            n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n            last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n            lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n            lora_path: Path to a LoRA file to apply to the model.\n            verbose: Print verbose output to stderr.\n        Raises:\n            ValueError: If the model path does not exist.\n        Returns:\n            A Llama instance.\n        \"\"\"\nself.verbose = verbose\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_parts = n_parts\nself.params.n_gpu_layers = n_gpu_layers\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mmap = use_mmap if lora_path is None else False\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.last_n_tokens_size = last_n_tokens_size\nself.n_batch = min(n_ctx, n_batch)\nself.eval_tokens: Deque[llama_cpp.llama_token] = deque(maxlen=n_ctx)\nself.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx if logits_all else 1)\nself.cache: Optional[LlamaCache] = None\nself.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\nself.lora_base = lora_base\nself.lora_path = lora_path\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\nassert self.ctx is not None\nif self.lora_path:\nif llama_cpp.llama_apply_lora_from_file(\nself.ctx,\nllama_cpp.c_char_p(self.lora_path.encode(\"utf-8\")),\nllama_cpp.c_char_p(self.lora_base.encode(\"utf-8\"))\nif self.lora_base is not None\nelse llama_cpp.c_char_p(0),\nllama_cpp.c_int(self.n_threads),\n):\nraise RuntimeError(\nf\"Failed to apply LoRA from lora path: {self.lora_path} to base path: {self.lora_base}\"\n)\nif self.verbose:\nprint(llama_cpp.llama_print_system_info().decode(\"utf-8\"), file=sys.stderr)\ndef tokenize(\nself, text: bytes, add_bos: bool = True\n) -&gt; List[llama_cpp.llama_token]:\n\"\"\"Tokenize a string.\n        Args:\n            text: The utf-8 encoded string to tokenize.\n        Raises:\n            RuntimeError: If the tokenization failed.\n        Returns:\n            A list of tokens.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = llama_cpp.llama_n_ctx(self.ctx)\ntokens = (llama_cpp.llama_token * int(n_ctx))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nn_ctx,\nllama_cpp.c_bool(add_bos),\n)\nif int(n_tokens) &lt; 0:\nn_tokens = abs(n_tokens)\ntokens = (llama_cpp.llama_token * int(n_tokens))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nllama_cpp.c_int(n_tokens),\nllama_cpp.c_bool(add_bos),\n)\nif n_tokens &lt; 0:\nraise RuntimeError(\nf'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}'\n)\nreturn list(tokens[:n_tokens])\ndef detokenize(self, tokens: List[llama_cpp.llama_token]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n        Args:\n            tokens: The list of tokens to detokenize.\n        Returns:\n            The detokenized string.\n        \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(self.ctx, token)\nreturn output\ndef set_cache(self, cache: Optional[LlamaCache]):\n\"\"\"Set the cache.\n        Args:\n            cache: The cache to set.\n        \"\"\"\nself.cache = cache\ndef reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.eval_tokens.clear()\nself.eval_logits.clear()\ndef eval(self, tokens: Sequence[llama_cpp.llama_token]):\n\"\"\"Evaluate a list of tokens.\n        Args:\n            tokens: The list of tokens to evaluate.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), len(self.eval_tokens))\nn_tokens = len(batch)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(n_tokens),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif int(return_code) != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\n# Save tokens\nself.eval_tokens.extend(batch)\n# Save logits\nrows = n_tokens if self.params.logits_all else 1\nn_vocab = llama_cpp.llama_n_vocab(self.ctx)\ncols = int(n_vocab)\nlogits_view = llama_cpp.llama_get_logits(self.ctx)\nlogits: List[List[float]] = [\n[logits_view[i * cols + j] for j in range(cols)] for i in range(rows)\n]\nself.eval_logits.extend(logits)\ndef _sample(\nself,\nlast_n_tokens_data,  # type: llama_cpp.Array[llama_cpp.llama_token]\nlast_n_tokens_size: llama_cpp.c_int,\ntop_k: llama_cpp.c_int,\ntop_p: llama_cpp.c_float,\ntemp: llama_cpp.c_float,\ntfs_z: llama_cpp.c_float,\nrepeat_penalty: llama_cpp.c_float,\nfrequency_penalty: llama_cpp.c_float,\npresence_penalty: llama_cpp.c_float,\nmirostat_mode: llama_cpp.c_int,\nmirostat_tau: llama_cpp.c_float,\nmirostat_eta: llama_cpp.c_float,\npenalize_nl: bool = True,\n):\nassert self.ctx is not None\nassert len(self.eval_logits) &gt; 0\nn_vocab = int(llama_cpp.llama_n_vocab(self.ctx))\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\ntop_k = llama_cpp.c_int(n_vocab) if top_k.value &lt;= 0 else top_k\nlast_n_tokens_size = (\nllama_cpp.c_int(n_ctx)\nif last_n_tokens_size.value &lt; 0\nelse last_n_tokens_size\n)\nlogits = self.eval_logits[-1]\nnl_logit = logits[int(Llama.token_nl())]\ndata = (llama_cpp.llama_token_data * n_vocab)(\n*[\nllama_cpp.llama_token_data(\nid=llama_cpp.llama_token(i),\nlogit=logits[i],\np=llama_cpp.c_float(0.0),\n)\nfor i in range(n_vocab)\n]\n)\nsize = llama_cpp.c_size_t(n_vocab)\nsorted = False\ncandidates = llama_cpp.llama_token_data_array(\ndata=data,\nsize=size,\nsorted=sorted,\n)\nllama_cpp.llama_sample_repetition_penalty(\nctx=self.ctx,\nlast_tokens_data=last_n_tokens_data,\nlast_tokens_size=last_n_tokens_size,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\npenalty=repeat_penalty,\n)\nllama_cpp.llama_sample_frequency_and_presence_penalties(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\nlast_tokens_data=last_n_tokens_data,\nlast_tokens_size=last_n_tokens_size,\nalpha_frequency=frequency_penalty,\nalpha_presence=presence_penalty,\n)\nif not penalize_nl:\ncandidates.data[int(Llama.token_nl())].logit = nl_logit\nif temp.value == 0.0:\nreturn llama_cpp.llama_sample_token_greedy(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\n)\nelif mirostat_mode.value == 1:\nmirostat_mu = llama_cpp.c_float(2.0 * mirostat_tau.value)\nmirostat_m = llama_cpp.c_int(100)\nllama_cpp.llama_sample_temperature(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntemp=temp,\n)\nreturn llama_cpp.llama_sample_token_mirostat(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntau=mirostat_tau,\neta=mirostat_eta,\nmu=llama_cpp.ctypes.byref(mirostat_mu),  # type: ignore\nm=mirostat_m,\n)\nelif mirostat_mode.value == 2:\nmirostat_mu = llama_cpp.c_float(2.0 * mirostat_tau.value)\nllama_cpp.llama_sample_temperature(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.pointer(candidates),\ntemp=temp,\n)\nreturn llama_cpp.llama_sample_token_mirostat_v2(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntau=mirostat_tau,\neta=mirostat_eta,\nmu=llama_cpp.ctypes.byref(mirostat_mu),  # type: ignore\n)\nelse:\nllama_cpp.llama_sample_top_k(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\nk=top_k,\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_tail_free(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\nz=tfs_z,\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_typical(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\np=llama_cpp.c_float(1.0),\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_top_p(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\np=top_p,\nmin_keep=llama_cpp.c_size_t(1),\n)\nllama_cpp.llama_sample_temperature(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\ntemp=temp,\n)\nreturn llama_cpp.llama_sample_token(\nctx=self.ctx,\ncandidates=llama_cpp.ctypes.byref(candidates),  # type: ignore\n)\ndef sample(\nself,\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_eta: float = 0.1,\nmirostat_tau: float = 5.0,\npenalize_nl: bool = True,\n):\n\"\"\"Sample a token from the model.\n        Args:\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n        Returns:\n            The sampled token.\n        \"\"\"\nassert self.ctx is not None\nlast_n_tokens_data = [llama_cpp.llama_token(0)] * max(\n0, self.last_n_tokens_size - len(self.eval_tokens)\n) + list(self.eval_tokens)[-self.last_n_tokens_size :]\nreturn self._sample(\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\ntfs_z=llama_cpp.c_float(tfs_z),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\nfrequency_penalty=llama_cpp.c_float(frequency_penalty),\npresence_penalty=llama_cpp.c_float(presence_penalty),\nmirostat_mode=llama_cpp.c_int(mirostat_mode),\nmirostat_tau=llama_cpp.c_float(mirostat_tau),\nmirostat_eta=llama_cpp.c_float(mirostat_eta),\npenalize_nl=penalize_nl,\n)\ndef generate(\nself,\ntokens: Sequence[llama_cpp.llama_token],\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nreset: bool = True,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\n) -&gt; Generator[\nllama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None\n]:\n\"\"\"Create a generator of tokens from a prompt.\n        Examples:\n            &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n            &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n            &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n            ...     print(llama.detokenize([token]))\n        Args:\n            tokens: The prompt tokens.\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n            reset: Whether to reset the model state.\n        Yields:\n            The generated tokens.\n        \"\"\"\nassert self.ctx is not None\nif reset and len(self.eval_tokens) &gt; 0:\nlongest_prefix = 0\nfor a, b in zip(self.eval_tokens, tokens[:-1]):\nif a == b:\nlongest_prefix += 1\nelse:\nbreak\nif longest_prefix &gt; 0:\nif self.verbose:\nprint(\"Llama.generate: prefix-match hit\", file=sys.stderr)\nreset = False\ntokens = tokens[longest_prefix:]\nfor _ in range(len(self.eval_tokens) - longest_prefix):\nself.eval_tokens.pop()\ntry:\nself.eval_logits.pop()\nexcept IndexError:\npass\nif reset:\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\n)\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\ndef create_embedding(self, input: str, model: Optional[str] = None) -&gt; Embedding:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            An embedding object.\n        \"\"\"\nassert self.ctx is not None\nmodel_name: str = model if model is not None else self.model_path\nif self.params.embedding == False:\nraise RuntimeError(\n\"Llama model must be created with embedding=True to call this method\"\n)\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nreturn {\n\"object\": \"list\",\n\"data\": [\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": 0,\n}\n],\n\"model\": model_name,\n\"usage\": {\n\"prompt_tokens\": n_tokens,\n\"total_tokens\": n_tokens,\n},\n}\ndef embed(self, input: str) -&gt; List[float]:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            A list of embeddings\n        \"\"\"\nreturn list(map(float, self.create_embedding(input)[\"data\"][0][\"embedding\"]))\ndef _create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 16,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[Iterator[Completion], Iterator[CompletionChunk]]:\nassert self.ctx is not None\ncompletion_id: str = f\"cmpl-{str(uuid.uuid4())}\"\ncreated: int = int(time.time())\ncompletion_tokens: List[llama_cpp.llama_token] = []\n# Add blank space to start of prompt to match OG llama tokenizer\nprompt_tokens: List[llama_cpp.llama_token] = self.tokenize(\nb\" \" + prompt.encode(\"utf-8\")\n)\ntext: bytes = b\"\"\nreturned_characters: int = 0\nstop = stop if stop is not None else []\nmodel_name: str = model if model is not None else self.model_path\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\nif len(prompt_tokens) + max_tokens &gt; int(llama_cpp.llama_n_ctx(self.ctx)):\nraise ValueError(\nf\"Requested tokens exceed context window of {llama_cpp.llama_n_ctx(self.ctx)}\"\n)\nif stop != []:\nstop_sequences = [s.encode(\"utf-8\") for s in stop]\nelse:\nstop_sequences = []\nif logprobs is not None and self.params.logits_all is False:\nraise ValueError(\n\"logprobs is not supported for models created with logits_all=False\"\n)\nif self.cache:\ntry:\ncache_item = self.cache[prompt_tokens]\ncache_prefix_len = Llama.longest_token_prefix(\ncache_item.eval_tokens, prompt_tokens\n)\neval_prefix_len = Llama.longest_token_prefix(\nself.eval_tokens, prompt_tokens\n)\nif cache_prefix_len &gt; eval_prefix_len:\nself.load_state(cache_item)\nif self.verbose:\nprint(\"Llama._create_completion: cache hit\", file=sys.stderr)\nexcept KeyError:\nif self.verbose:\nprint(\"Llama._create_completion: cache miss\", file=sys.stderr)\nfinish_reason = \"length\"\nmultibyte_fix = 0\nfor token in self.generate(\nprompt_tokens,\ntop_k=top_k,\ntop_p=top_p,\ntemp=temperature,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\n):\nif token == Llama.token_eos():\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"stop\"\nbreak\ncompletion_tokens.append(token)\nall_text = self.detokenize(completion_tokens)\n# Contains multi-byte UTF8\nfor k, char in enumerate(all_text[-3:]):\nk = 3 - k\nfor num, pattern in [(2, 192), (3, 224), (4, 240)]:\n# Bitwise AND check\nif num &gt; k and pattern &amp; char == pattern:\nmultibyte_fix = num - k\n# Stop incomplete bytes from passing\nif multibyte_fix &gt; 0:\nmultibyte_fix -= 1\ncontinue\nany_stop = [s for s in stop_sequences if s in all_text]\nif len(any_stop) &gt; 0:\nfirst_stop = any_stop[0]\ntext = all_text[: all_text.index(first_stop)]\nfinish_reason = \"stop\"\nbreak\nif stream:\nstart = returned_characters\nlongest = 0\n# We want to avoid yielding any characters from\n# the generated text if they are part of a stop\n# sequence.\nfor s in stop_sequences:\nfor i in range(len(s), 0, -1):\nif all_text.endswith(s[:i]):\nif i &gt; longest:\nlongest = i\nbreak\ntext = all_text[: len(all_text) - longest]\nreturned_characters += len(text[start:])\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": text[start:].decode(\"utf-8\", errors=\"ignore\"),\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": None,\n}\n],\n}\nif len(completion_tokens) &gt;= max_tokens:\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"length\"\nbreak\nif self.cache:\nif self.verbose:\nprint(\"Llama._create_completion: cache save\", file=sys.stderr)\nself.cache[prompt_tokens + completion_tokens] = self.save_state()\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nif stream:\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": text[returned_characters:].decode(\n\"utf-8\", errors=\"ignore\"\n),\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": finish_reason,\n}\n],\n}\nreturn\ntext_str = text.decode(\"utf-8\", errors=\"ignore\")\nif echo:\ntext_str = prompt + text_str\nif suffix is not None:\ntext_str = text_str + suffix\nlogprobs_or_none: Optional[CompletionLogprobs] = None\nif logprobs is not None:\ntext_offset = 0\ntext_offsets: List[int] = []\ntoken_logprobs: List[float] = []\ntokens: List[str] = []\ntop_logprobs: List[Dict[str, float]] = []\nall_tokens = prompt_tokens + completion_tokens\nall_token_strs = [\nself.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\nfor token in all_tokens\n]\nall_logprobs = [\nLlama.logits_to_logprobs(list(map(float, row)))\nfor row in self.eval_logits\n]\nfor token, token_str, logprobs_token in zip(\nall_tokens, all_token_strs, all_logprobs\n):\ntext_offsets.append(text_offset)\ntext_offset += len(token_str)\ntokens.append(token_str)\nsorted_logprobs = list(\nsorted(\nzip(logprobs_token, range(len(logprobs_token))), reverse=True\n)\n)\ntoken_logprobs.append(sorted_logprobs[int(token)][0])\ntop_logprob = {\nself.detokenize([llama_cpp.llama_token(i)]).decode(\n\"utf-8\", errors=\"ignore\"\n): logprob\nfor logprob, i in sorted_logprobs[:logprobs]\n}\ntop_logprob.update({token_str: sorted_logprobs[int(token)][0]})\ntop_logprobs.append(top_logprob)\nlogprobs_or_none = {\n\"tokens\": tokens,\n\"text_offset\": text_offsets,\n\"token_logprobs\": token_logprobs,\n\"top_logprobs\": top_logprobs,\n}\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": model_name,\n\"choices\": [\n{\n\"text\": text_str,\n\"index\": 0,\n\"logprobs\": logprobs_or_none,\n\"finish_reason\": finish_reason,\n}\n],\n\"usage\": {\n\"prompt_tokens\": len(prompt_tokens),\n\"completion_tokens\": len(completion_tokens),\n\"total_tokens\": len(prompt_tokens) + len(completion_tokens),\n},\n}\ndef create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\ndef __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\n)\ndef _convert_text_completion_to_chat(\nself, completion: Completion\n) -&gt; ChatCompletion:\nreturn {\n\"id\": \"chat\" + completion[\"id\"],\n\"object\": \"chat.completion\",\n\"created\": completion[\"created\"],\n\"model\": completion[\"model\"],\n\"choices\": [\n{\n\"index\": 0,\n\"message\": {\n\"role\": \"assistant\",\n\"content\": completion[\"choices\"][0][\"text\"],\n},\n\"finish_reason\": completion[\"choices\"][0][\"finish_reason\"],\n}\n],\n\"usage\": completion[\"usage\"],\n}\ndef _convert_text_completion_chunks_to_chat(\nself,\nchunks: Iterator[CompletionChunk],\n) -&gt; Iterator[ChatCompletionChunk]:\nfor i, chunk in enumerate(chunks):\nif i == 0:\nyield {\n\"id\": \"chat\" + chunk[\"id\"],\n\"model\": chunk[\"model\"],\n\"created\": chunk[\"created\"],\n\"object\": \"chat.completion.chunk\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"role\": \"assistant\",\n},\n\"finish_reason\": None,\n}\n],\n}\nyield {\n\"id\": \"chat\" + chunk[\"id\"],\n\"model\": chunk[\"model\"],\n\"created\": chunk[\"created\"],\n\"object\": \"chat.completion.chunk\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"content\": chunk[\"choices\"][0][\"text\"],\n},\n\"finish_reason\": chunk[\"choices\"][0][\"finish_reason\"],\n}\n],\n}\ndef create_chat_completion(\nself,\nmessages: List[ChatCompletionMessage],\ntemperature: float = 0.2,\ntop_p: float = 0.95,\ntop_k: int = 40,\nstream: bool = False,\nstop: Optional[List[str]] = [],\nmax_tokens: int = 256,\npresence_penalty: float = 0.0,\nfrequency_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n\"\"\"Generate a chat completion from a list of messages.\n        Args:\n            messages: A list of messages to generate a response for.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n            stop: A list of strings to stop generation when encountered.\n            max_tokens: The maximum number of tokens to generate.\n            repeat_penalty: The penalty to apply to repeated tokens.\n        Returns:\n            Generated chat completion or a stream of chat completion chunks.\n        \"\"\"\nstop = stop if stop is not None else []\nchat_history = \"\".join(\nf'### {\"Human\" if message[\"role\"] == \"user\" else \"Assistant\"}:{message[\"content\"]}'\nfor message in messages\n)\nPROMPT = chat_history + \"### Assistant:\"\nPROMPT_STOP = [\"### Assistant:\", \"### Human:\"]\ncompletion_or_chunks = self(\nprompt=PROMPT,\nstop=PROMPT_STOP + stop,\ntemperature=temperature,\ntop_p=top_p,\ntop_k=top_k,\nstream=stream,\nmax_tokens=max_tokens,\nrepeat_penalty=repeat_penalty,\npresence_penalty=presence_penalty,\nfrequency_penalty=frequency_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_chunks_to_chat(chunks)\nelse:\ncompletion: Completion = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_to_chat(completion)\ndef __del__(self):\nif self.ctx is not None:\nllama_cpp.llama_free(self.ctx)\nself.ctx = None\ndef __getstate__(self):\nreturn dict(\nverbose=self.verbose,\nmodel_path=self.model_path,\nn_ctx=self.params.n_ctx,\nn_parts=self.params.n_parts,\nn_gpu_layers=self.params.n_gpu_layers,\nseed=self.params.seed,\nf16_kv=self.params.f16_kv,\nlogits_all=self.params.logits_all,\nvocab_only=self.params.vocab_only,\nuse_mmap=self.params.use_mmap,\nuse_mlock=self.params.use_mlock,\nembedding=self.params.embedding,\nlast_n_tokens_size=self.last_n_tokens_size,\nn_batch=self.n_batch,\nn_threads=self.n_threads,\nlora_base=self.lora_base,\nlora_path=self.lora_path,\n)\ndef __setstate__(self, state):\nself.__init__(\nmodel_path=state[\"model_path\"],\nn_ctx=state[\"n_ctx\"],\nn_parts=state[\"n_parts\"],\nn_gpu_layers=state[\"n_gpu_layers\"],\nseed=state[\"seed\"],\nf16_kv=state[\"f16_kv\"],\nlogits_all=state[\"logits_all\"],\nvocab_only=state[\"vocab_only\"],\nuse_mmap=state[\"use_mmap\"],\nuse_mlock=state[\"use_mlock\"],\nembedding=state[\"embedding\"],\nn_threads=state[\"n_threads\"],\nn_batch=state[\"n_batch\"],\nlast_n_tokens_size=state[\"last_n_tokens_size\"],\nlora_base=state[\"lora_base\"],\nlora_path=state[\"lora_path\"],\nverbose=state[\"verbose\"],\n)\ndef save_state(self) -&gt; LlamaState:\nassert self.ctx is not None\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nllama_state = (llama_cpp.c_uint8 * int(state_size))()\nn_bytes = llama_cpp.llama_copy_state_data(self.ctx, llama_state)\nif int(n_bytes) &gt; int(state_size):\nraise RuntimeError(\"Failed to copy llama state data\")\nllama_state_compact = (llama_cpp.c_uint8 * int(n_bytes))()\nllama_cpp.ctypes.memmove(llama_state_compact, llama_state, int(n_bytes))\nif self.verbose:\nprint(\nf\"Llama.save_state: saving {n_bytes} bytes of llama state\",\nfile=sys.stderr,\n)\nreturn LlamaState(\neval_tokens=self.eval_tokens.copy(),\neval_logits=self.eval_logits.copy(),\nllama_state=llama_state_compact,\nllama_state_size=n_bytes,\n)\ndef load_state(self, state: LlamaState) -&gt; None:\nassert self.ctx is not None\nself.eval_tokens = state.eval_tokens.copy()\nself.eval_logits = state.eval_logits.copy()\nstate_size = state.llama_state_size\nif llama_cpp.llama_set_state_data(self.ctx, state.llama_state) != state_size:\nraise RuntimeError(\"Failed to set llama state data\")\n@staticmethod\ndef token_eos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n@staticmethod\ndef token_bos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n@staticmethod\ndef token_nl() -&gt; llama_cpp.llama_token:\n\"\"\"Return the newline token.\"\"\"\nreturn llama_cpp.llama_token_nl()\n@staticmethod\ndef logits_to_logprobs(logits: List[float]) -&gt; List[float]:\nexps = [math.exp(float(x)) for x in logits]\nsum_exps = sum(exps)\nreturn [math.log(x / sum_exps) for x in exps]\n@staticmethod\ndef longest_token_prefix(\na: Sequence[llama_cpp.llama_token], b: Sequence[llama_cpp.llama_token]\n):\nlongest_prefix = 0\nfor _a, _b in zip(a, b):\nif _a == _b:\nlongest_prefix += 1\nelse:\nbreak\nreturn longest_prefix\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__init__","title":"<code>__init__(model_path, n_ctx=512, n_parts=-1, n_gpu_layers=0, seed=1337, f16_kv=True, logits_all=False, vocab_only=False, use_mmap=True, use_mlock=False, embedding=False, n_threads=None, n_batch=512, last_n_tokens_size=64, lora_base=None, lora_path=None, verbose=True)</code>","text":"<p>Load a llama.cpp model from <code>model_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>n_ctx</code> <code>int</code> <p>Maximum context size.</p> <code>512</code> <code>n_parts</code> <code>int</code> <p>Number of parts to split the model into. If -1, the number of parts is automatically determined.</p> <code>-1</code> <code>seed</code> <code>int</code> <p>Random seed. 0 for random.</p> <code>1337</code> <code>f16_kv</code> <code>bool</code> <p>Use half-precision for key/value cache.</p> <code>True</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens, not just the last token.</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>Only load the vocabulary no weights.</p> <code>False</code> <code>use_mmap</code> <code>bool</code> <p>Use mmap if possible.</p> <code>True</code> <code>use_mlock</code> <code>bool</code> <p>Force the system to keep the model in RAM.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Embedding mode only.</p> <code>False</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads to use. If None, the number of threads is automatically determined.</p> <code>None</code> <code>n_batch</code> <code>int</code> <p>Maximum number of prompt tokens to batch together when calling llama_eval.</p> <code>512</code> <code>last_n_tokens_size</code> <code>int</code> <p>Maximum number of tokens to keep in the last_n_tokens deque.</p> <code>64</code> <code>lora_base</code> <code>Optional[str]</code> <p>Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.</p> <code>None</code> <code>lora_path</code> <code>Optional[str]</code> <p>Path to a LoRA file to apply to the model.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print verbose output to stderr.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model path does not exist.</p> <p>Returns:</p> Type Description <p>A Llama instance.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nn_gpu_layers: int = 0,\nseed: int = 1337,\nf16_kv: bool = True,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mmap: bool = True,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 512,\nlast_n_tokens_size: int = 64,\nlora_base: Optional[str] = None,\nlora_path: Optional[str] = None,\nverbose: bool = True,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n    Args:\n        model_path: Path to the model.\n        n_ctx: Maximum context size.\n        n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n        seed: Random seed. 0 for random.\n        f16_kv: Use half-precision for key/value cache.\n        logits_all: Return logits for all tokens, not just the last token.\n        vocab_only: Only load the vocabulary no weights.\n        use_mmap: Use mmap if possible.\n        use_mlock: Force the system to keep the model in RAM.\n        embedding: Embedding mode only.\n        n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n        n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n        last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n        lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n        lora_path: Path to a LoRA file to apply to the model.\n        verbose: Print verbose output to stderr.\n    Raises:\n        ValueError: If the model path does not exist.\n    Returns:\n        A Llama instance.\n    \"\"\"\nself.verbose = verbose\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_parts = n_parts\nself.params.n_gpu_layers = n_gpu_layers\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mmap = use_mmap if lora_path is None else False\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.last_n_tokens_size = last_n_tokens_size\nself.n_batch = min(n_ctx, n_batch)\nself.eval_tokens: Deque[llama_cpp.llama_token] = deque(maxlen=n_ctx)\nself.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx if logits_all else 1)\nself.cache: Optional[LlamaCache] = None\nself.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\nself.lora_base = lora_base\nself.lora_path = lora_path\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\nassert self.ctx is not None\nif self.lora_path:\nif llama_cpp.llama_apply_lora_from_file(\nself.ctx,\nllama_cpp.c_char_p(self.lora_path.encode(\"utf-8\")),\nllama_cpp.c_char_p(self.lora_base.encode(\"utf-8\"))\nif self.lora_base is not None\nelse llama_cpp.c_char_p(0),\nllama_cpp.c_int(self.n_threads),\n):\nraise RuntimeError(\nf\"Failed to apply LoRA from lora path: {self.lora_path} to base path: {self.lora_base}\"\n)\nif self.verbose:\nprint(llama_cpp.llama_print_system_info().decode(\"utf-8\"), file=sys.stderr)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.tokenize","title":"<code>tokenize(text, add_bos=True)</code>","text":"<p>Tokenize a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>bytes</code> <p>The utf-8 encoded string to tokenize.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the tokenization failed.</p> <p>Returns:</p> Type Description <code>List[llama_cpp.llama_token]</code> <p>A list of tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def tokenize(\nself, text: bytes, add_bos: bool = True\n) -&gt; List[llama_cpp.llama_token]:\n\"\"\"Tokenize a string.\n    Args:\n        text: The utf-8 encoded string to tokenize.\n    Raises:\n        RuntimeError: If the tokenization failed.\n    Returns:\n        A list of tokens.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = llama_cpp.llama_n_ctx(self.ctx)\ntokens = (llama_cpp.llama_token * int(n_ctx))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nn_ctx,\nllama_cpp.c_bool(add_bos),\n)\nif int(n_tokens) &lt; 0:\nn_tokens = abs(n_tokens)\ntokens = (llama_cpp.llama_token * int(n_tokens))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nllama_cpp.c_int(n_tokens),\nllama_cpp.c_bool(add_bos),\n)\nif n_tokens &lt; 0:\nraise RuntimeError(\nf'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}'\n)\nreturn list(tokens[:n_tokens])\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.detokenize","title":"<code>detokenize(tokens)</code>","text":"<p>Detokenize a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[llama_cpp.llama_token]</code> <p>The list of tokens to detokenize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The detokenized string.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def detokenize(self, tokens: List[llama_cpp.llama_token]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n    Args:\n        tokens: The list of tokens to detokenize.\n    Returns:\n        The detokenized string.\n    \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(self.ctx, token)\nreturn output\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.reset","title":"<code>reset()</code>","text":"<p>Reset the model state.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.eval_tokens.clear()\nself.eval_logits.clear()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.eval","title":"<code>eval(tokens)</code>","text":"<p>Evaluate a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[llama_cpp.llama_token]</code> <p>The list of tokens to evaluate.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def eval(self, tokens: Sequence[llama_cpp.llama_token]):\n\"\"\"Evaluate a list of tokens.\n    Args:\n        tokens: The list of tokens to evaluate.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), len(self.eval_tokens))\nn_tokens = len(batch)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(n_tokens),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif int(return_code) != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\n# Save tokens\nself.eval_tokens.extend(batch)\n# Save logits\nrows = n_tokens if self.params.logits_all else 1\nn_vocab = llama_cpp.llama_n_vocab(self.ctx)\ncols = int(n_vocab)\nlogits_view = llama_cpp.llama_get_logits(self.ctx)\nlogits: List[List[float]] = [\n[logits_view[i * cols + j] for j in range(cols)] for i in range(rows)\n]\nself.eval_logits.extend(logits)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.sample","title":"<code>sample(top_k=40, top_p=0.95, temp=0.8, repeat_penalty=1.1, frequency_penalty=0.0, presence_penalty=0.0, tfs_z=1.0, mirostat_mode=0, mirostat_eta=0.1, mirostat_tau=5.0, penalize_nl=True)</code>","text":"<p>Sample a token from the model.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> <code>40</code> <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> <code>0.95</code> <code>temp</code> <code>float</code> <p>The temperature parameter.</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> <code>1.1</code> <p>Returns:</p> Type Description <p>The sampled token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def sample(\nself,\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_eta: float = 0.1,\nmirostat_tau: float = 5.0,\npenalize_nl: bool = True,\n):\n\"\"\"Sample a token from the model.\n    Args:\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n    Returns:\n        The sampled token.\n    \"\"\"\nassert self.ctx is not None\nlast_n_tokens_data = [llama_cpp.llama_token(0)] * max(\n0, self.last_n_tokens_size - len(self.eval_tokens)\n) + list(self.eval_tokens)[-self.last_n_tokens_size :]\nreturn self._sample(\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\ntfs_z=llama_cpp.c_float(tfs_z),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\nfrequency_penalty=llama_cpp.c_float(frequency_penalty),\npresence_penalty=llama_cpp.c_float(presence_penalty),\nmirostat_mode=llama_cpp.c_int(mirostat_mode),\nmirostat_tau=llama_cpp.c_float(mirostat_tau),\nmirostat_eta=llama_cpp.c_float(mirostat_eta),\npenalize_nl=penalize_nl,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.generate","title":"<code>generate(tokens, top_k=40, top_p=0.95, temp=0.8, repeat_penalty=1.1, reset=True, frequency_penalty=0.0, presence_penalty=0.0, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1)</code>","text":"<p>Create a generator of tokens from a prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n&gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n&gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n...     print(llama.detokenize([token]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[llama_cpp.llama_token]</code> <p>The prompt tokens.</p> required <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> <code>40</code> <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> <code>0.95</code> <code>temp</code> <code>float</code> <p>The temperature parameter.</p> <code>0.8</code> <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> <code>1.1</code> <code>reset</code> <code>bool</code> <p>Whether to reset the model state.</p> <code>True</code> <p>Yields:</p> Type Description <code>Generator[llama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None]</code> <p>The generated tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def generate(\nself,\ntokens: Sequence[llama_cpp.llama_token],\ntop_k: int = 40,\ntop_p: float = 0.95,\ntemp: float = 0.80,\nrepeat_penalty: float = 1.1,\nreset: bool = True,\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\n) -&gt; Generator[\nllama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None\n]:\n\"\"\"Create a generator of tokens from a prompt.\n    Examples:\n        &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n        &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n        &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n        ...     print(llama.detokenize([token]))\n    Args:\n        tokens: The prompt tokens.\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n        reset: Whether to reset the model state.\n    Yields:\n        The generated tokens.\n    \"\"\"\nassert self.ctx is not None\nif reset and len(self.eval_tokens) &gt; 0:\nlongest_prefix = 0\nfor a, b in zip(self.eval_tokens, tokens[:-1]):\nif a == b:\nlongest_prefix += 1\nelse:\nbreak\nif longest_prefix &gt; 0:\nif self.verbose:\nprint(\"Llama.generate: prefix-match hit\", file=sys.stderr)\nreset = False\ntokens = tokens[longest_prefix:]\nfor _ in range(len(self.eval_tokens) - longest_prefix):\nself.eval_tokens.pop()\ntry:\nself.eval_logits.pop()\nexcept IndexError:\npass\nif reset:\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\n)\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_embedding","title":"<code>create_embedding(input, model=None)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>An embedding object.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_embedding(self, input: str, model: Optional[str] = None) -&gt; Embedding:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        An embedding object.\n    \"\"\"\nassert self.ctx is not None\nmodel_name: str = model if model is not None else self.model_path\nif self.params.embedding == False:\nraise RuntimeError(\n\"Llama model must be created with embedding=True to call this method\"\n)\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nreturn {\n\"object\": \"list\",\n\"data\": [\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": 0,\n}\n],\n\"model\": model_name,\n\"usage\": {\n\"prompt_tokens\": n_tokens,\n\"total_tokens\": n_tokens,\n},\n}\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.embed","title":"<code>embed(input)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>A list of embeddings</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def embed(self, input: str) -&gt; List[float]:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        A list of embeddings\n    \"\"\"\nreturn list(map(float, self.create_embedding(input)[\"data\"][0][\"embedding\"]))\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_completion","title":"<code>create_completion(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], frequency_penalty=0.0, presence_penalty=0.0, repeat_penalty=1.1, top_k=40, stream=False, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>Optional[List[str]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Iterator[CompletionChunk]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__call__","title":"<code>__call__(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], frequency_penalty=0.0, presence_penalty=0.0, repeat_penalty=1.1, top_k=40, stream=False, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>Optional[List[str]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Iterator[CompletionChunk]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nfrequency_penalty: float = 0.0,\npresence_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nfrequency_penalty=frequency_penalty,\npresence_penalty=presence_penalty,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_chat_completion","title":"<code>create_chat_completion(messages, temperature=0.2, top_p=0.95, top_k=40, stream=False, stop=[], max_tokens=256, presence_penalty=0.0, frequency_penalty=0.0, repeat_penalty=1.1, tfs_z=1.0, mirostat_mode=0, mirostat_tau=5.0, mirostat_eta=0.1, model=None)</code>","text":"<p>Generate a chat completion from a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatCompletionMessage]</code> <p>A list of messages to generate a response for.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.2</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <code>stop</code> <code>Optional[List[str]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>256</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>Union[ChatCompletion, Iterator[ChatCompletionChunk]]</code> <p>Generated chat completion or a stream of chat completion chunks.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_chat_completion(\nself,\nmessages: List[ChatCompletionMessage],\ntemperature: float = 0.2,\ntop_p: float = 0.95,\ntop_k: int = 40,\nstream: bool = False,\nstop: Optional[List[str]] = [],\nmax_tokens: int = 256,\npresence_penalty: float = 0.0,\nfrequency_penalty: float = 0.0,\nrepeat_penalty: float = 1.1,\ntfs_z: float = 1.0,\nmirostat_mode: int = 0,\nmirostat_tau: float = 5.0,\nmirostat_eta: float = 0.1,\nmodel: Optional[str] = None,\n) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n\"\"\"Generate a chat completion from a list of messages.\n    Args:\n        messages: A list of messages to generate a response for.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n        stop: A list of strings to stop generation when encountered.\n        max_tokens: The maximum number of tokens to generate.\n        repeat_penalty: The penalty to apply to repeated tokens.\n    Returns:\n        Generated chat completion or a stream of chat completion chunks.\n    \"\"\"\nstop = stop if stop is not None else []\nchat_history = \"\".join(\nf'### {\"Human\" if message[\"role\"] == \"user\" else \"Assistant\"}:{message[\"content\"]}'\nfor message in messages\n)\nPROMPT = chat_history + \"### Assistant:\"\nPROMPT_STOP = [\"### Assistant:\", \"### Human:\"]\ncompletion_or_chunks = self(\nprompt=PROMPT,\nstop=PROMPT_STOP + stop,\ntemperature=temperature,\ntop_p=top_p,\ntop_k=top_k,\nstream=stream,\nmax_tokens=max_tokens,\nrepeat_penalty=repeat_penalty,\npresence_penalty=presence_penalty,\nfrequency_penalty=frequency_penalty,\ntfs_z=tfs_z,\nmirostat_mode=mirostat_mode,\nmirostat_tau=mirostat_tau,\nmirostat_eta=mirostat_eta,\nmodel=model,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_chunks_to_chat(chunks)\nelse:\ncompletion: Completion = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_to_chat(completion)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.set_cache","title":"<code>set_cache(cache)</code>","text":"<p>Set the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>Optional[LlamaCache]</code> <p>The cache to set.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def set_cache(self, cache: Optional[LlamaCache]):\n\"\"\"Set the cache.\n    Args:\n        cache: The cache to set.\n    \"\"\"\nself.cache = cache\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.save_state","title":"<code>save_state()</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>def save_state(self) -&gt; LlamaState:\nassert self.ctx is not None\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nllama_state = (llama_cpp.c_uint8 * int(state_size))()\nn_bytes = llama_cpp.llama_copy_state_data(self.ctx, llama_state)\nif int(n_bytes) &gt; int(state_size):\nraise RuntimeError(\"Failed to copy llama state data\")\nllama_state_compact = (llama_cpp.c_uint8 * int(n_bytes))()\nllama_cpp.ctypes.memmove(llama_state_compact, llama_state, int(n_bytes))\nif self.verbose:\nprint(\nf\"Llama.save_state: saving {n_bytes} bytes of llama state\",\nfile=sys.stderr,\n)\nreturn LlamaState(\neval_tokens=self.eval_tokens.copy(),\neval_logits=self.eval_logits.copy(),\nllama_state=llama_state_compact,\nllama_state_size=n_bytes,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.load_state","title":"<code>load_state(state)</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>def load_state(self, state: LlamaState) -&gt; None:\nassert self.ctx is not None\nself.eval_tokens = state.eval_tokens.copy()\nself.eval_logits = state.eval_logits.copy()\nstate_size = state.llama_state_size\nif llama_cpp.llama_set_state_data(self.ctx, state.llama_state) != state_size:\nraise RuntimeError(\"Failed to set llama state data\")\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_bos","title":"<code>token_bos()</code>  <code>staticmethod</code>","text":"<p>Return the beginning-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_bos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_eos","title":"<code>token_eos()</code>  <code>staticmethod</code>","text":"<p>Return the end-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_eos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.LlamaCache","title":"<code>llama_cpp.LlamaCache</code>","text":"<p>Cache for a llama.cpp model.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>class LlamaCache:\n\"\"\"Cache for a llama.cpp model.\"\"\"\ndef __init__(self, capacity_bytes: int = (2 &lt;&lt; 30)):\nself.cache_state: OrderedDict[\nTuple[llama_cpp.llama_token, ...], \"LlamaState\"\n] = OrderedDict()\nself.capacity_bytes = capacity_bytes\n@property\ndef cache_size(self):\nreturn sum([state.llama_state_size for state in self.cache_state.values()])\ndef _find_longest_prefix_key(\nself,\nkey: Tuple[llama_cpp.llama_token, ...],\n) -&gt; Optional[Tuple[llama_cpp.llama_token, ...]]:\nmin_len = 0\nmin_key = None\nkeys = (\n(k, Llama.longest_token_prefix(k, key)) for k in self.cache_state.keys()\n)\nfor k, prefix_len in keys:\nif prefix_len &gt; min_len:\nmin_len = prefix_len\nmin_key = k\nreturn min_key\ndef __getitem__(self, key: Sequence[llama_cpp.llama_token]) -&gt; \"LlamaState\":\nkey = tuple(key)\n_key = self._find_longest_prefix_key(key)\nif _key is None:\nraise KeyError(f\"Key not found\")\nvalue = self.cache_state[_key]\nself.cache_state.move_to_end(_key)\nreturn value\ndef __contains__(self, key: Sequence[llama_cpp.llama_token]) -&gt; bool:\nreturn self._find_longest_prefix_key(tuple(key)) is not None\ndef __setitem__(self, key: Sequence[llama_cpp.llama_token], value: \"LlamaState\"):\nkey = tuple(key)\nif key in self.cache_state:\ndel self.cache_state[key]\nself.cache_state[key] = value\nwhile self.cache_size &gt; self.capacity_bytes:\nself.cache_state.popitem(last=False)\n</code></pre>"},{"location":"#llama_cpp.LlamaState","title":"<code>llama_cpp.LlamaState</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>class LlamaState:\ndef __init__(\nself,\neval_tokens: Deque[llama_cpp.llama_token],\neval_logits: Deque[List[float]],\nllama_state,  # type: llama_cpp.Array[llama_cpp.c_uint8]\nllama_state_size: int,\n):\nself.eval_tokens = eval_tokens\nself.eval_logits = eval_logits\nself.llama_state = llama_state\nself.llama_state_size = llama_state_size\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC","title":"<code>LLAMA_FILE_MAGIC = b'ggjt'</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_MAGIC_UNVERSIONED","title":"<code>LLAMA_FILE_MAGIC_UNVERSIONED = b'ggml'</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FILE_VERSION","title":"<code>LLAMA_FILE_VERSION = c_int(2)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_ALL_F32","title":"<code>LLAMA_FTYPE_ALL_F32 = c_int(0)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_F16","title":"<code>LLAMA_FTYPE_MOSTLY_F16 = c_int(1)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_0 = c_int(2)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_1","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_1 = c_int(3)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = c_int(4)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q5_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q5_0 = c_int(8)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q5_1","title":"<code>LLAMA_FTYPE_MOSTLY_Q5_1 = c_int(9)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q8_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q8_0 = c_int(7)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_SESSION_MAGIC","title":"<code>LLAMA_SESSION_MAGIC = b'ggsn'</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_SESSION_VERSION","title":"<code>LLAMA_SESSION_VERSION = c_int(1)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.c_float_p","title":"<code>c_float_p = POINTER(c_float)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.c_size_t_p","title":"<code>c_size_t_p = POINTER(c_size_t)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.c_uint8_p","title":"<code>c_uint8_p = POINTER(c_uint8)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_p","title":"<code>llama_context_p = c_void_p</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params_p","title":"<code>llama_context_params_p = POINTER(llama_context_params)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_progress_callback","title":"<code>llama_progress_callback = ctypes.CFUNCTYPE(None, c_float, c_void_p)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token","title":"<code>llama_token = c_int</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_data_array_p","title":"<code>llama_token_data_array_p = POINTER(llama_token_data_array)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_data_p","title":"<code>llama_token_data_p = POINTER(llama_token_data)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_p","title":"<code>llama_token_p = POINTER(llama_token)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params","title":"<code>llama_context_params</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_context_params(Structure):\n_fields_ = [\n(\"n_ctx\", c_int),  # text context\n(\"n_parts\", c_int),  # -1 for default\n(\"n_gpu_layers\", c_int),  # number of layers to store in VRAM\n(\"seed\", c_int),  # RNG seed, 0 for random\n(\"f16_kv\", c_bool),  # use fp16 for KV cache\n(\n\"logits_all\",\nc_bool,\n),  # the llama_eval() call computes all logits, not just the last one\n(\"vocab_only\", c_bool),  # only load the vocabulary, no weights\n(\"use_mmap\", c_bool),  # use mmap if possible\n(\"use_mlock\", c_bool),  # force system to keep model in RAM\n(\"embedding\", c_bool),  # embedding mode only\n# called with a progress value between 0 and 1, pass NULL to disable\n(\"progress_callback\", llama_progress_callback),\n# context pointer passed to the progress callback\n(\"progress_callback_user_data\", c_void_p),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_data","title":"<code>llama_token_data</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_token_data(Structure):\n_fields_ = [\n(\"id\", llama_token),  # token id\n(\"logit\", c_float),  # log-odds of the token\n(\"p\", c_float),  # probability of the token\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_data_array","title":"<code>llama_token_data_array</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_token_data_array(Structure):\n_fields_ = [\n(\"data\", llama_token_data_p),\n(\"size\", c_size_t),\n(\"sorted\", c_bool),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_apply_lora_from_file","title":"<code>llama_apply_lora_from_file(ctx, path_lora, path_base_model, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_apply_lora_from_file(\nctx: llama_context_p,\npath_lora: c_char_p,\npath_base_model: c_char_p,\nn_threads: c_int,\n) -&gt; c_int:\nreturn _lib.llama_apply_lora_from_file(ctx, path_lora, path_base_model, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_context_default_params","title":"<code>llama_context_default_params()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_context_default_params() -&gt; llama_context_params:\nreturn _lib.llama_context_default_params()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_copy_state_data","title":"<code>llama_copy_state_data(ctx, dst)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_copy_state_data(\nctx: llama_context_p, dst  # type: Array[c_uint8]\n) -&gt; int:\nreturn _lib.llama_copy_state_data(ctx, dst)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_eval","title":"<code>llama_eval(ctx, tokens, n_tokens, n_past, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_eval(\nctx: llama_context_p,\ntokens,  # type: Array[llama_token]\nn_tokens: c_int,\nn_past: c_int,\nn_threads: c_int,\n) -&gt; c_int:\nreturn _lib.llama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_free","title":"<code>llama_free(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_free(ctx: llama_context_p):\n_lib.llama_free(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_embeddings","title":"<code>llama_get_embeddings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_embeddings(\nctx: llama_context_p,\n):  # type: (...) -&gt; Array[float] # type: ignore\nreturn _lib.llama_get_embeddings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_kv_cache_token_count","title":"<code>llama_get_kv_cache_token_count(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_kv_cache_token_count(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_get_kv_cache_token_count(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_logits","title":"<code>llama_get_logits(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_logits(\nctx: llama_context_p,\n):  # type: (...) -&gt; Array[float] # type: ignore\nreturn _lib.llama_get_logits(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_state_size","title":"<code>llama_get_state_size(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_state_size(ctx: llama_context_p) -&gt; c_size_t:\nreturn _lib.llama_get_state_size(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_init_from_file","title":"<code>llama_init_from_file(path_model, params)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_init_from_file(\npath_model: bytes, params: llama_context_params\n) -&gt; llama_context_p:\nreturn _lib.llama_init_from_file(path_model, params)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_load_session_file","title":"<code>llama_load_session_file(ctx, path_session, tokens_out, n_token_capacity, n_token_count_out)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_load_session_file(\nctx: llama_context_p,\npath_session: bytes,\ntokens_out,  # type: Array[llama_token]\nn_token_capacity: c_size_t,\nn_token_count_out,  # type: _Pointer[c_size_t]\n) -&gt; c_size_t:\nreturn _lib.llama_load_session_file(\nctx, path_session, tokens_out, n_token_capacity, n_token_count_out\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_mlock_supported","title":"<code>llama_mlock_supported()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_mlock_supported() -&gt; bool:\nreturn _lib.llama_mlock_supported()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_mmap_supported","title":"<code>llama_mmap_supported()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_mmap_supported() -&gt; bool:\nreturn _lib.llama_mmap_supported()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_model_quantize","title":"<code>llama_model_quantize(fname_inp, fname_out, ftype, nthread)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_model_quantize(\nfname_inp: bytes, fname_out: bytes, ftype: c_int, nthread: c_int\n) -&gt; c_int:\nreturn _lib.llama_model_quantize(fname_inp, fname_out, ftype, nthread)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_ctx","title":"<code>llama_n_ctx(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_ctx(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_ctx(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_embd","title":"<code>llama_n_embd(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_embd(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_embd(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_vocab","title":"<code>llama_n_vocab(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_vocab(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_vocab(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_system_info","title":"<code>llama_print_system_info()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_system_info() -&gt; bytes:\nreturn _lib.llama_print_system_info()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_timings","title":"<code>llama_print_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_timings(ctx: llama_context_p):\n_lib.llama_print_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_reset_timings","title":"<code>llama_reset_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_reset_timings(ctx: llama_context_p):\n_lib.llama_reset_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_frequency_and_presence_penalties","title":"<code>llama_sample_frequency_and_presence_penalties(ctx, candidates, last_tokens_data, last_tokens_size, alpha_frequency, alpha_presence)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_frequency_and_presence_penalties(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nlast_tokens_data,  # type: Array[llama_token]\nlast_tokens_size: c_int,\nalpha_frequency: c_float,\nalpha_presence: c_float,\n):\nreturn _lib.llama_sample_frequency_and_presence_penalties(\nctx,\ncandidates,\nlast_tokens_data,\nlast_tokens_size,\nalpha_frequency,\nalpha_presence,\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_repetition_penalty","title":"<code>llama_sample_repetition_penalty(ctx, candidates, last_tokens_data, last_tokens_size, penalty)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_repetition_penalty(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nlast_tokens_data,  # type: Array[llama_token]\nlast_tokens_size: c_int,\npenalty: c_float,\n):\nreturn _lib.llama_sample_repetition_penalty(\nctx, candidates, last_tokens_data, last_tokens_size, penalty\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_softmax","title":"<code>llama_sample_softmax(ctx, candidates)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_softmax(\nctx: llama_context_p, candidates  # type: _Pointer[llama_token_data]\n):\nreturn _lib.llama_sample_softmax(ctx, candidates)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_tail_free","title":"<code>llama_sample_tail_free(ctx, candidates, z, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_tail_free(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nz: c_float,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_tail_free(ctx, candidates, z, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_temperature","title":"<code>llama_sample_temperature(ctx, candidates, temp)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_temperature(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\ntemp: c_float,\n):\nreturn _lib.llama_sample_temperature(ctx, candidates, temp)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token","title":"<code>llama_sample_token(ctx, candidates)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\n) -&gt; llama_token:\nreturn _lib.llama_sample_token(ctx, candidates)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token_greedy","title":"<code>llama_sample_token_greedy(ctx, candidates)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token_greedy(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\n) -&gt; llama_token:\nreturn _lib.llama_sample_token_greedy(ctx, candidates)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token_mirostat","title":"<code>llama_sample_token_mirostat(ctx, candidates, tau, eta, m, mu)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token_mirostat(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\ntau: c_float,\neta: c_float,\nm: c_int,\nmu,  # type: _Pointer[c_float]\n) -&gt; llama_token:\nreturn _lib.llama_sample_token_mirostat(ctx, candidates, tau, eta, m, mu)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_token_mirostat_v2","title":"<code>llama_sample_token_mirostat_v2(ctx, candidates, tau, eta, mu)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_token_mirostat_v2(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\ntau: c_float,\neta: c_float,\nmu,  # type: _Pointer[c_float]\n) -&gt; llama_token:\nreturn _lib.llama_sample_token_mirostat_v2(ctx, candidates, tau, eta, mu)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_top_k","title":"<code>llama_sample_top_k(ctx, candidates, k, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_top_k(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\nk: c_int,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_top_k(ctx, candidates, k, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_top_p","title":"<code>llama_sample_top_p(ctx, candidates, p, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_top_p(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\np: c_float,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_top_p(ctx, candidates, p, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_typical","title":"<code>llama_sample_typical(ctx, candidates, p, min_keep)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_typical(\nctx: llama_context_p,\ncandidates,  # type: _Pointer[llama_token_data_array]\np: c_float,\nmin_keep: c_size_t,\n):\nreturn _lib.llama_sample_typical(ctx, candidates, p, min_keep)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_save_session_file","title":"<code>llama_save_session_file(ctx, path_session, tokens, n_token_count)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_save_session_file(\nctx: llama_context_p,\npath_session: bytes,\ntokens,  # type: Array[llama_token]\nn_token_count: c_size_t,\n) -&gt; c_size_t:\nreturn _lib.llama_save_session_file(ctx, path_session, tokens, n_token_count)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_rng_seed","title":"<code>llama_set_rng_seed(ctx, seed)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_rng_seed(ctx: llama_context_p, seed: c_int):\nreturn _lib.llama_set_rng_seed(ctx, seed)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_state_data","title":"<code>llama_set_state_data(ctx, src)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_state_data(\nctx: llama_context_p, src  # type: Array[c_uint8]\n) -&gt; int:\nreturn _lib.llama_set_state_data(ctx, src)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_bos","title":"<code>llama_token_bos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_bos() -&gt; llama_token:\nreturn _lib.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_eos","title":"<code>llama_token_eos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_eos() -&gt; llama_token:\nreturn _lib.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_nl","title":"<code>llama_token_nl()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_nl() -&gt; llama_token:\nreturn _lib.llama_token_nl()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_to_str","title":"<code>llama_token_to_str(ctx, token)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_to_str(ctx: llama_context_p, token: llama_token) -&gt; bytes:\nreturn _lib.llama_token_to_str(ctx, token)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_tokenize","title":"<code>llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_tokenize(\nctx: llama_context_p,\ntext: bytes,\ntokens,  # type: Array[llama_token]\nn_max_tokens: c_int,\nadd_bos: c_bool,\n) -&gt; int:\nreturn _lib.llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"}]}