{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#python-bindings-for-llamacpp","title":"\ud83e\udd99 Python Bindings for <code>llama.cpp</code>","text":"<p>Simple Python bindings for @ggerganov's <code>llama.cpp</code> library. This package provides:</p> <ul> <li>Low-level access to C API via <code>ctypes</code> interface.</li> <li>High-level Python API for text completion</li> <li>OpenAI-like API</li> <li>LangChain compatibility</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install llama-cpp-python\n</code></pre>"},{"location":"#high-level-api","title":"High-level API","text":"<pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; llm = Llama(model_path=\"./models/7B/ggml-model.bin\")\n&gt;&gt;&gt; output = llm(\"Q: Name the planets in the solar system? A: \", max_tokens=32, stop=[\"Q:\", \"\\n\"], echo=True)\n&gt;&gt;&gt; print(output)\n{\n\"id\": \"cmpl-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\"object\": \"text_completion\",\n\"created\": 1679561337,\n\"model\": \"./models/7B/ggml-model.bin\",\n\"choices\": [\n{\n\"text\": \"Q: Name the planets in the solar system? A: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.\",\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": \"stop\"\n}\n],\n\"usage\": {\n\"prompt_tokens\": 14,\n\"completion_tokens\": 28,\n\"total_tokens\": 42\n}\n}\n</code></pre>"},{"location":"#web-server","title":"Web Server","text":"<p><code>llama-cpp-python</code> offers a web server which aims to act as a drop-in replacement for the OpenAI API. This allows you to use llama.cpp compatible models with any OpenAI compatible client (language libraries, services, etc).</p> <p>To install the server package and get started:</p> <pre><code>pip install llama-cpp-python[server]\nexport MODEL=./models/7B/ggml-model.bin\npython3 -m llama_cpp.server\n</code></pre> <p>Navigate to http://localhost:8000/docs to see the OpenAPI documentation.</p>"},{"location":"#low-level-api","title":"Low-level API","text":"<p>The low-level API is a direct <code>ctypes</code> binding to the C API provided by <code>llama.cpp</code>. The entire API can be found in llama_cpp/llama_cpp.py and should mirror llama.h.</p>"},{"location":"#development","title":"Development","text":"<p>This package is under active development and I welcome any contributions.</p> <p>To get started, clone the repository and install the package in development mode:</p> <pre><code>git clone git@github.com:abetlen/llama-cpp-python.git\ngit submodule update --init --recursive\n# Will need to be re-run any time vendor/llama.cpp is updated\npython3 setup.py develop\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>Cache for a llama.cpp model.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>class LlamaCache:\n\"\"\"Cache for a llama.cpp model.\"\"\"\ndef __init__(self):\nself.cache_state: Dict[Tuple[llama_cpp.llama_token, ...], \"LlamaState\"] = dict()\ndef _sorted_keys(self) -&gt; List[Tuple[llama_cpp.llama_token, ...]]:\nreturn [\nkey\nfor _, key in sorted(\n((len(key), key) for key in self.cache_state.keys()), reverse=True\n)\n]\ndef _find_key(\nself, key: Tuple[llama_cpp.llama_token, ...]\n) -&gt; Optional[Tuple[llama_cpp.llama_token, ...]]:\nfor k in self._sorted_keys():\nif key[: len(k)] == k:\nreturn k\nreturn None\ndef __getitem__(\nself, key: Sequence[llama_cpp.llama_token]\n) -&gt; Optional[\"LlamaState\"]:\n_key = self._find_key(tuple(key))\nif _key is None:\nreturn None\nreturn self.cache_state[_key]\ndef __contains__(self, key: Sequence[llama_cpp.llama_token]) -&gt; bool:\nreturn self._find_key(tuple(key)) is not None\ndef __setitem__(self, key: Sequence[llama_cpp.llama_token], value: \"LlamaState\"):\nself.cache_state = dict()  # NOTE: Currently limit to one cache entry.\nself.cache_state[tuple(key)] = value\n</code></pre> Source code in <code>llama_cpp/llama.py</code> <pre><code>class LlamaState:\ndef __init__(\nself,\neval_tokens: Deque[llama_cpp.llama_token],\neval_logits: Deque[List[float]],\nllama_state,\n):\nself.eval_tokens = eval_tokens\nself.eval_logits = eval_logits\nself.llama_state = llama_state\n</code></pre>"},{"location":"#llama_cpp.Llama","title":"<code>llama_cpp.Llama</code>","text":"<p>High-level Python wrapper for a llama.cpp model.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>class Llama:\n\"\"\"High-level Python wrapper for a llama.cpp model.\"\"\"\ndef __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nseed: int = 1337,\nf16_kv: bool = True,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mmap: bool = True,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 512,\nlast_n_tokens_size: int = 64,\nlora_base: Optional[str] = None,\nlora_path: Optional[str] = None,\nverbose: bool = True,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n        Args:\n            model_path: Path to the model.\n            n_ctx: Maximum context size.\n            n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n            seed: Random seed. 0 for random.\n            f16_kv: Use half-precision for key/value cache.\n            logits_all: Return logits for all tokens, not just the last token.\n            vocab_only: Only load the vocabulary no weights.\n            use_mmap: Use mmap if possible.\n            use_mlock: Force the system to keep the model in RAM.\n            embedding: Embedding mode only.\n            n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n            n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n            last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n            lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n            lora_path: Path to a LoRA file to apply to the model.\n            verbose: Print verbose output to stderr.\n        Raises:\n            ValueError: If the model path does not exist.\n        Returns:\n            A Llama instance.\n        \"\"\"\nself.verbose = verbose\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_parts = n_parts\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mmap = use_mmap if lora_path is None else False\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.last_n_tokens_size = last_n_tokens_size\nself.n_batch = min(n_ctx, n_batch)\nself.eval_tokens: Deque[llama_cpp.llama_token] = deque(maxlen=n_ctx)\nself.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx)\nself.cache: Optional[LlamaCache] = None\nself.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\nself.lora_base = lora_base\nself.lora_path = lora_path\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\nassert self.ctx is not None\nif self.lora_path:\nif llama_cpp.llama_apply_lora_from_file(\nself.ctx,\nllama_cpp.c_char_p(self.lora_path.encode(\"utf-8\")),\nllama_cpp.c_char_p(self.lora_base.encode(\"utf-8\"))\nif self.lora_base is not None\nelse llama_cpp.c_char_p(0),\nllama_cpp.c_int(self.n_threads),\n):\nraise RuntimeError(\nf\"Failed to apply LoRA from lora path: {self.lora_path} to base path: {self.lora_base}\"\n)\nif self.verbose:\nprint(llama_cpp.llama_print_system_info().decode(\"utf-8\"), file=sys.stderr)\ndef tokenize(self, text: bytes) -&gt; List[llama_cpp.llama_token]:\n\"\"\"Tokenize a string.\n        Args:\n            text: The utf-8 encoded string to tokenize.\n        Raises:\n            RuntimeError: If the tokenization failed.\n        Returns:\n            A list of tokens.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = llama_cpp.llama_n_ctx(self.ctx)\ntokens = (llama_cpp.llama_token * int(n_ctx))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nn_ctx,\nllama_cpp.c_bool(True),\n)\nif int(n_tokens) &lt; 0:\nraise RuntimeError(f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}')\nreturn list(tokens[:n_tokens])\ndef detokenize(self, tokens: List[llama_cpp.llama_token]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n        Args:\n            tokens: The list of tokens to detokenize.\n        Returns:\n            The detokenized string.\n        \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(self.ctx, token)\nreturn output\ndef set_cache(self, cache: Optional[LlamaCache]):\n\"\"\"Set the cache.\n        Args:\n            cache: The cache to set.\n        \"\"\"\nself.cache = cache\ndef reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.eval_tokens.clear()\nself.eval_logits.clear()\ndef eval(self, tokens: Sequence[llama_cpp.llama_token]):\n\"\"\"Evaluate a list of tokens.\n        Args:\n            tokens: The list of tokens to evaluate.\n        \"\"\"\nassert self.ctx is not None\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), len(self.eval_tokens))\nn_tokens = len(batch)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(n_tokens),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif int(return_code) != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\nself.eval_tokens.extend(batch)\nif self.params.logits_all:\nn_vocab = llama_cpp.llama_n_vocab(self.ctx)\ncols = int(n_vocab)\nrows = n_tokens\nlogits_view = llama_cpp.llama_get_logits(self.ctx)\nlogits = [\n[logits_view[i * cols + j] for j in range(cols)]\nfor i in range(rows)\n]\nself.eval_logits.extend(logits)\ndef sample(\nself,\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\n):\n\"\"\"Sample a token from the model.\n        Args:\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n        Returns:\n            The sampled token.\n        \"\"\"\nassert self.ctx is not None\nlast_n_tokens_data = [llama_cpp.llama_token(0)] * max(\n0, self.last_n_tokens_size - len(self.eval_tokens)\n) + list(self.eval_tokens)[-self.last_n_tokens_size :]\nreturn llama_cpp.llama_sample_top_p_top_k(\nctx=self.ctx,\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\n)\ndef generate(\nself,\ntokens: Sequence[llama_cpp.llama_token],\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\nreset: bool = True,\n) -&gt; Generator[\nllama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None\n]:\n\"\"\"Create a generator of tokens from a prompt.\n        Examples:\n            &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n            &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n            &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n            ...     print(llama.detokenize([token]))\n        Args:\n            tokens: The prompt tokens.\n            top_k: The top-k sampling parameter.\n            top_p: The top-p sampling parameter.\n            temp: The temperature parameter.\n            repeat_penalty: The repeat penalty parameter.\n            reset: Whether to reset the model state.\n        Yields:\n            The generated tokens.\n        \"\"\"\nassert self.ctx is not None\nif (\nreset\nand len(self.eval_tokens) &gt; 0\nand tuple(self.eval_tokens) == tuple(tokens[: len(self.eval_tokens)])\n):\nif self.verbose:\nprint(\"generate cache hit\", file=sys.stderr)\nreset = False\ntokens = tokens[len(self.eval_tokens) :]\nif reset:\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\n)\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\ndef create_embedding(self, input: str) -&gt; Embedding:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            An embedding object.\n        \"\"\"\nassert self.ctx is not None\nif self.params.embedding == False:\nraise RuntimeError(\n\"Llama model must be created with embedding=True to call this method\"\n)\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nreturn {\n\"object\": \"list\",\n\"data\": [\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": 0,\n}\n],\n\"model\": self.model_path,\n\"usage\": {\n\"prompt_tokens\": n_tokens,\n\"total_tokens\": n_tokens,\n},\n}\ndef embed(self, input: str) -&gt; List[float]:\n\"\"\"Embed a string.\n        Args:\n            input: The utf-8 encoded string to embed.\n        Returns:\n            A list of embeddings\n        \"\"\"\nreturn list(map(float, self.create_embedding(input)[\"data\"][0][\"embedding\"]))\ndef _create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 16,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Iterator[Completion], Iterator[CompletionChunk]]:\nassert self.ctx is not None\ncompletion_id: str = f\"cmpl-{str(uuid.uuid4())}\"\ncreated: int = int(time.time())\ncompletion_tokens: List[llama_cpp.llama_token] = []\n# Add blank space to start of prompt to match OG llama tokenizer\nprompt_tokens: List[llama_cpp.llama_token] = self.tokenize(\nb\" \" + prompt.encode(\"utf-8\")\n)\ntext: bytes = b\"\"\nreturned_characters: int = 0\nstop = stop if stop is not None else []\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\nif len(prompt_tokens) + max_tokens &gt; int(llama_cpp.llama_n_ctx(self.ctx)):\nraise ValueError(\nf\"Requested tokens exceed context window of {llama_cpp.llama_n_ctx(self.ctx)}\"\n)\nif stop != []:\nstop_sequences = [s.encode(\"utf-8\") for s in stop]\nelse:\nstop_sequences = []\nif logprobs is not None and self.params.logits_all is False:\nraise ValueError(\n\"logprobs is not supported for models created with logits_all=False\"\n)\nif self.cache and prompt_tokens in self.cache:\nif self.verbose:\nprint(\"cache hit\", file=sys.stderr)\nself.load_state(self.cache[prompt_tokens])\nfinish_reason = \"length\"\nfor token in self.generate(\nprompt_tokens,\ntop_k=top_k,\ntop_p=top_p,\ntemp=temperature,\nrepeat_penalty=repeat_penalty,\n):\nif token == llama_cpp.llama_token_eos():\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"stop\"\nbreak\nif self.cache and len(completion_tokens) == 0:\nif prompt_tokens not in self.cache:\nif self.verbose:\nprint(\"cache miss\", file=sys.stderr)\nself.cache[prompt_tokens] = self.save_state()\ncompletion_tokens.append(token)\nall_text = self.detokenize(completion_tokens)\nany_stop = [s for s in stop_sequences if s in all_text]\nif len(any_stop) &gt; 0:\nfirst_stop = any_stop[0]\ntext = all_text[: all_text.index(first_stop)]\nfinish_reason = \"stop\"\nbreak\nif stream:\nstart = returned_characters\nlongest = 0\n# We want to avoid yielding any characters from\n# the generated text if they are part of a stop\n# sequence.\nfor s in stop_sequences:\nfor i in range(len(s), 0, -1):\nif all_text.endswith(s[:i]):\nif i &gt; longest:\nlongest = i\nbreak\ntext = all_text[: len(all_text) - longest]\nreturned_characters += len(text[start:])\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": self.model_path,\n\"choices\": [\n{\n\"text\": text[start:].decode(\"utf-8\"),\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": None,\n}\n],\n}\nif len(completion_tokens) &gt;= max_tokens:\ntext = self.detokenize(completion_tokens)\nfinish_reason = \"length\"\nbreak\nif stream:\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": self.model_path,\n\"choices\": [\n{\n\"text\": text[returned_characters:].decode(\"utf-8\"),\n\"index\": 0,\n\"logprobs\": None,\n\"finish_reason\": finish_reason,\n}\n],\n}\nreturn\ntext_str = text.decode(\"utf-8\")\nif echo:\ntext_str = prompt + text_str\nif suffix is not None:\ntext_str = text_str + suffix\nlogprobs_or_none: Optional[CompletionLogprobs] = None\nif logprobs is not None:\ntext_offset = 0\ntext_offsets: List[int] = []\ntoken_logprobs: List[float] = []\ntokens: List[str] = []\ntop_logprobs: List[Dict[str, float]] = []\nall_tokens = prompt_tokens + completion_tokens\nall_token_strs = [\nself.detokenize([token]).decode(\"utf-8\") for token in all_tokens\n]\nall_logprobs = [\n[Llama.logit_to_logprob(logit) for logit in row]\nfor row in self.eval_logits\n]\nfor token, token_str, logprobs_token in zip(\nall_tokens, all_token_strs, all_logprobs\n):\ntext_offsets.append(text_offset)\ntext_offset += len(token_str)\ntokens.append(token_str)\nsorted_logprobs = list(\nsorted(\nzip(logprobs_token, range(len(logprobs_token))), reverse=True\n)\n)\ntoken_logprobs.append(sorted_logprobs[int(token)][0])\ntop_logprob = {\nself.detokenize([llama_cpp.llama_token(i)]).decode(\"utf-8\"): logprob\nfor logprob, i in sorted_logprobs[:logprobs]\n}\ntop_logprob.update({token_str: sorted_logprobs[int(token)][0]})\ntop_logprobs.append(top_logprob)\nlogprobs_or_none = {\n\"tokens\": tokens,\n\"text_offset\": text_offsets,\n\"token_logprobs\": token_logprobs,\n\"top_logprobs\": top_logprobs,\n}\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nyield {\n\"id\": completion_id,\n\"object\": \"text_completion\",\n\"created\": created,\n\"model\": self.model_path,\n\"choices\": [\n{\n\"text\": text_str,\n\"index\": 0,\n\"logprobs\": logprobs_or_none,\n\"finish_reason\": finish_reason,\n}\n],\n\"usage\": {\n\"prompt_tokens\": len(prompt_tokens),\n\"completion_tokens\": len(completion_tokens),\n\"total_tokens\": len(prompt_tokens) + len(completion_tokens),\n},\n}\ndef create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\ndef __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n        Args:\n            prompt: The prompt to generate text from.\n            suffix: A suffix to append to the generated text. If None, no suffix is appended.\n            max_tokens: The maximum number of tokens to generate.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            logprobs: The number of logprobs to return. If None, no logprobs are returned.\n            echo: Whether to echo the prompt.\n            stop: A list of strings to stop generation when encountered.\n            repeat_penalty: The penalty to apply to repeated tokens.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n        Raises:\n            ValueError: If the requested tokens exceed the context window.\n            RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n        Returns:\n            Response object containing the generated text.\n        \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\ndef _convert_text_completion_to_chat(\nself, completion: Completion\n) -&gt; ChatCompletion:\nreturn {\n\"id\": \"chat\" + completion[\"id\"],\n\"object\": \"chat.completion\",\n\"created\": completion[\"created\"],\n\"model\": completion[\"model\"],\n\"choices\": [\n{\n\"index\": 0,\n\"message\": {\n\"role\": \"assistant\",\n\"content\": completion[\"choices\"][0][\"text\"],\n},\n\"finish_reason\": completion[\"choices\"][0][\"finish_reason\"],\n}\n],\n\"usage\": completion[\"usage\"],\n}\ndef _convert_text_completion_chunks_to_chat(\nself,\nchunks: Iterator[CompletionChunk],\n) -&gt; Iterator[ChatCompletionChunk]:\nfor i, chunk in enumerate(chunks):\nif i == 0:\nyield {\n\"id\": \"chat\" + chunk[\"id\"],\n\"model\": chunk[\"model\"],\n\"created\": chunk[\"created\"],\n\"object\": \"chat.completion.chunk\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"role\": \"assistant\",\n},\n\"finish_reason\": None,\n}\n],\n}\nyield {\n\"id\": \"chat\" + chunk[\"id\"],\n\"model\": chunk[\"model\"],\n\"created\": chunk[\"created\"],\n\"object\": \"chat.completion.chunk\",\n\"choices\": [\n{\n\"index\": 0,\n\"delta\": {\n\"content\": chunk[\"choices\"][0][\"text\"],\n},\n\"finish_reason\": chunk[\"choices\"][0][\"finish_reason\"],\n}\n],\n}\ndef create_chat_completion(\nself,\nmessages: List[ChatCompletionMessage],\ntemperature: float = 0.2,\ntop_p: float = 0.95,\ntop_k: int = 40,\nstream: bool = False,\nstop: Optional[List[str]] = [],\nmax_tokens: int = 256,\nrepeat_penalty: float = 1.1,\n) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n\"\"\"Generate a chat completion from a list of messages.\n        Args:\n            messages: A list of messages to generate a response for.\n            temperature: The temperature to use for sampling.\n            top_p: The top-p value to use for sampling.\n            top_k: The top-k value to use for sampling.\n            stream: Whether to stream the results.\n            stop: A list of strings to stop generation when encountered.\n            max_tokens: The maximum number of tokens to generate.\n            repeat_penalty: The penalty to apply to repeated tokens.\n        Returns:\n            Generated chat completion or a stream of chat completion chunks.\n        \"\"\"\nstop = stop if stop is not None else []\nchat_history = \"\".join(\nf'### {\"Human\" if message[\"role\"] == \"user\" else \"Assistant\"}:{message[\"content\"]}'\nfor message in messages\n)\nPROMPT = chat_history + \"### Assistant:\"\nPROMPT_STOP = [\"### Assistant:\", \"### Human:\"]\ncompletion_or_chunks = self(\nprompt=PROMPT,\nstop=PROMPT_STOP + stop,\ntemperature=temperature,\ntop_p=top_p,\ntop_k=top_k,\nstream=stream,\nmax_tokens=max_tokens,\nrepeat_penalty=repeat_penalty,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_chunks_to_chat(chunks)\nelse:\ncompletion: Completion = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_to_chat(completion)\ndef __del__(self):\nif self.ctx is not None:\nllama_cpp.llama_free(self.ctx)\nself.ctx = None\ndef __getstate__(self):\nreturn dict(\nverbose=self.verbose,\nmodel_path=self.model_path,\nn_ctx=self.params.n_ctx,\nn_parts=self.params.n_parts,\nseed=self.params.seed,\nf16_kv=self.params.f16_kv,\nlogits_all=self.params.logits_all,\nvocab_only=self.params.vocab_only,\nuse_mmap=self.params.use_mmap,\nuse_mlock=self.params.use_mlock,\nembedding=self.params.embedding,\nlast_n_tokens_size=self.last_n_tokens_size,\nn_batch=self.n_batch,\nn_threads=self.n_threads,\nlora_base=self.lora_base,\nlora_path=self.lora_path,\n)\ndef __setstate__(self, state):\nself.__init__(\nmodel_path=state[\"model_path\"],\nn_ctx=state[\"n_ctx\"],\nn_parts=state[\"n_parts\"],\nseed=state[\"seed\"],\nf16_kv=state[\"f16_kv\"],\nlogits_all=state[\"logits_all\"],\nvocab_only=state[\"vocab_only\"],\nuse_mmap=state[\"use_mmap\"],\nuse_mlock=state[\"use_mlock\"],\nembedding=state[\"embedding\"],\nn_threads=state[\"n_threads\"],\nn_batch=state[\"n_batch\"],\nlast_n_tokens_size=state[\"last_n_tokens_size\"],\nlora_base=state[\"lora_base\"],\nlora_path=state[\"lora_path\"],\nverbose=state[\"verbose\"],\n)\ndef save_state(self) -&gt; LlamaState:\nassert self.ctx is not None\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nllama_state = (llama_cpp.c_uint8 * int(state_size))()\nif llama_cpp.llama_copy_state_data(self.ctx, llama_state) != state_size:\nraise RuntimeError(\"Failed to copy llama state data\")\nreturn LlamaState(\neval_tokens=self.eval_tokens.copy(),\neval_logits=self.eval_logits.copy(),\nllama_state=llama_state,\n)\ndef load_state(self, state: LlamaState) -&gt; None:\nassert self.ctx is not None\nself.eval_tokens = state.eval_tokens.copy()\nself.eval_logits = state.eval_logits.copy()\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nif llama_cpp.llama_set_state_data(self.ctx, state.llama_state) != state_size:\nraise RuntimeError(\"Failed to set llama state data\")\n@staticmethod\ndef token_eos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n@staticmethod\ndef token_bos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n@staticmethod\ndef logit_to_logprob(x: float) -&gt; float:\nreturn math.log(1.0 + math.exp(x))\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__init__","title":"<code>__init__(model_path, n_ctx=512, n_parts=-1, seed=1337, f16_kv=True, logits_all=False, vocab_only=False, use_mmap=True, use_mlock=False, embedding=False, n_threads=None, n_batch=512, last_n_tokens_size=64, lora_base=None, lora_path=None, verbose=True)</code>","text":"<p>Load a llama.cpp model from <code>model_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>n_ctx</code> <code>int</code> <p>Maximum context size.</p> <code>512</code> <code>n_parts</code> <code>int</code> <p>Number of parts to split the model into. If -1, the number of parts is automatically determined.</p> <code>-1</code> <code>seed</code> <code>int</code> <p>Random seed. 0 for random.</p> <code>1337</code> <code>f16_kv</code> <code>bool</code> <p>Use half-precision for key/value cache.</p> <code>True</code> <code>logits_all</code> <code>bool</code> <p>Return logits for all tokens, not just the last token.</p> <code>False</code> <code>vocab_only</code> <code>bool</code> <p>Only load the vocabulary no weights.</p> <code>False</code> <code>use_mmap</code> <code>bool</code> <p>Use mmap if possible.</p> <code>True</code> <code>use_mlock</code> <code>bool</code> <p>Force the system to keep the model in RAM.</p> <code>False</code> <code>embedding</code> <code>bool</code> <p>Embedding mode only.</p> <code>False</code> <code>n_threads</code> <code>Optional[int]</code> <p>Number of threads to use. If None, the number of threads is automatically determined.</p> <code>None</code> <code>n_batch</code> <code>int</code> <p>Maximum number of prompt tokens to batch together when calling llama_eval.</p> <code>512</code> <code>last_n_tokens_size</code> <code>int</code> <p>Maximum number of tokens to keep in the last_n_tokens deque.</p> <code>64</code> <code>lora_base</code> <code>Optional[str]</code> <p>Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.</p> <code>None</code> <code>lora_path</code> <code>Optional[str]</code> <p>Path to a LoRA file to apply to the model.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print verbose output to stderr.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model path does not exist.</p> <p>Returns:</p> Type Description <p>A Llama instance.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __init__(\nself,\nmodel_path: str,\n# NOTE: These parameters are likely to change in the future.\nn_ctx: int = 512,\nn_parts: int = -1,\nseed: int = 1337,\nf16_kv: bool = True,\nlogits_all: bool = False,\nvocab_only: bool = False,\nuse_mmap: bool = True,\nuse_mlock: bool = False,\nembedding: bool = False,\nn_threads: Optional[int] = None,\nn_batch: int = 512,\nlast_n_tokens_size: int = 64,\nlora_base: Optional[str] = None,\nlora_path: Optional[str] = None,\nverbose: bool = True,\n):\n\"\"\"Load a llama.cpp model from `model_path`.\n    Args:\n        model_path: Path to the model.\n        n_ctx: Maximum context size.\n        n_parts: Number of parts to split the model into. If -1, the number of parts is automatically determined.\n        seed: Random seed. 0 for random.\n        f16_kv: Use half-precision for key/value cache.\n        logits_all: Return logits for all tokens, not just the last token.\n        vocab_only: Only load the vocabulary no weights.\n        use_mmap: Use mmap if possible.\n        use_mlock: Force the system to keep the model in RAM.\n        embedding: Embedding mode only.\n        n_threads: Number of threads to use. If None, the number of threads is automatically determined.\n        n_batch: Maximum number of prompt tokens to batch together when calling llama_eval.\n        last_n_tokens_size: Maximum number of tokens to keep in the last_n_tokens deque.\n        lora_base: Optional path to base model, useful if using a quantized base model and you want to apply LoRA to an f16 model.\n        lora_path: Path to a LoRA file to apply to the model.\n        verbose: Print verbose output to stderr.\n    Raises:\n        ValueError: If the model path does not exist.\n    Returns:\n        A Llama instance.\n    \"\"\"\nself.verbose = verbose\nself.model_path = model_path\nself.params = llama_cpp.llama_context_default_params()\nself.params.n_ctx = n_ctx\nself.params.n_parts = n_parts\nself.params.seed = seed\nself.params.f16_kv = f16_kv\nself.params.logits_all = logits_all\nself.params.vocab_only = vocab_only\nself.params.use_mmap = use_mmap if lora_path is None else False\nself.params.use_mlock = use_mlock\nself.params.embedding = embedding\nself.last_n_tokens_size = last_n_tokens_size\nself.n_batch = min(n_ctx, n_batch)\nself.eval_tokens: Deque[llama_cpp.llama_token] = deque(maxlen=n_ctx)\nself.eval_logits: Deque[List[float]] = deque(maxlen=n_ctx)\nself.cache: Optional[LlamaCache] = None\nself.n_threads = n_threads or max(multiprocessing.cpu_count() // 2, 1)\nself.lora_base = lora_base\nself.lora_path = lora_path\nif not os.path.exists(model_path):\nraise ValueError(f\"Model path does not exist: {model_path}\")\nself.ctx = llama_cpp.llama_init_from_file(\nself.model_path.encode(\"utf-8\"), self.params\n)\nassert self.ctx is not None\nif self.lora_path:\nif llama_cpp.llama_apply_lora_from_file(\nself.ctx,\nllama_cpp.c_char_p(self.lora_path.encode(\"utf-8\")),\nllama_cpp.c_char_p(self.lora_base.encode(\"utf-8\"))\nif self.lora_base is not None\nelse llama_cpp.c_char_p(0),\nllama_cpp.c_int(self.n_threads),\n):\nraise RuntimeError(\nf\"Failed to apply LoRA from lora path: {self.lora_path} to base path: {self.lora_base}\"\n)\nif self.verbose:\nprint(llama_cpp.llama_print_system_info().decode(\"utf-8\"), file=sys.stderr)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenize a string.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>bytes</code> <p>The utf-8 encoded string to tokenize.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the tokenization failed.</p> <p>Returns:</p> Type Description <code>List[llama_cpp.llama_token]</code> <p>A list of tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def tokenize(self, text: bytes) -&gt; List[llama_cpp.llama_token]:\n\"\"\"Tokenize a string.\n    Args:\n        text: The utf-8 encoded string to tokenize.\n    Raises:\n        RuntimeError: If the tokenization failed.\n    Returns:\n        A list of tokens.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = llama_cpp.llama_n_ctx(self.ctx)\ntokens = (llama_cpp.llama_token * int(n_ctx))()\nn_tokens = llama_cpp.llama_tokenize(\nself.ctx,\ntext,\ntokens,\nn_ctx,\nllama_cpp.c_bool(True),\n)\nif int(n_tokens) &lt; 0:\nraise RuntimeError(f'Failed to tokenize: text=\"{text}\" n_tokens={n_tokens}')\nreturn list(tokens[:n_tokens])\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.detokenize","title":"<code>detokenize(tokens)</code>","text":"<p>Detokenize a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>List[llama_cpp.llama_token]</code> <p>The list of tokens to detokenize.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The detokenized string.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def detokenize(self, tokens: List[llama_cpp.llama_token]) -&gt; bytes:\n\"\"\"Detokenize a list of tokens.\n    Args:\n        tokens: The list of tokens to detokenize.\n    Returns:\n        The detokenized string.\n    \"\"\"\nassert self.ctx is not None\noutput = b\"\"\nfor token in tokens:\noutput += llama_cpp.llama_token_to_str(self.ctx, token)\nreturn output\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.reset","title":"<code>reset()</code>","text":"<p>Reset the model state.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def reset(self):\n\"\"\"Reset the model state.\"\"\"\nself.eval_tokens.clear()\nself.eval_logits.clear()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.eval","title":"<code>eval(tokens)</code>","text":"<p>Evaluate a list of tokens.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[llama_cpp.llama_token]</code> <p>The list of tokens to evaluate.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def eval(self, tokens: Sequence[llama_cpp.llama_token]):\n\"\"\"Evaluate a list of tokens.\n    Args:\n        tokens: The list of tokens to evaluate.\n    \"\"\"\nassert self.ctx is not None\nn_ctx = int(llama_cpp.llama_n_ctx(self.ctx))\nfor i in range(0, len(tokens), self.n_batch):\nbatch = tokens[i : min(len(tokens), i + self.n_batch)]\nn_past = min(n_ctx - len(batch), len(self.eval_tokens))\nn_tokens = len(batch)\nreturn_code = llama_cpp.llama_eval(\nctx=self.ctx,\ntokens=(llama_cpp.llama_token * len(batch))(*batch),\nn_tokens=llama_cpp.c_int(n_tokens),\nn_past=llama_cpp.c_int(n_past),\nn_threads=llama_cpp.c_int(self.n_threads),\n)\nif int(return_code) != 0:\nraise RuntimeError(f\"llama_eval returned {return_code}\")\nself.eval_tokens.extend(batch)\nif self.params.logits_all:\nn_vocab = llama_cpp.llama_n_vocab(self.ctx)\ncols = int(n_vocab)\nrows = n_tokens\nlogits_view = llama_cpp.llama_get_logits(self.ctx)\nlogits = [\n[logits_view[i * cols + j] for j in range(cols)]\nfor i in range(rows)\n]\nself.eval_logits.extend(logits)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.sample","title":"<code>sample(top_k, top_p, temp, repeat_penalty)</code>","text":"<p>Sample a token from the model.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> required <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> required <code>temp</code> <code>float</code> <p>The temperature parameter.</p> required <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> required <p>Returns:</p> Type Description <p>The sampled token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def sample(\nself,\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\n):\n\"\"\"Sample a token from the model.\n    Args:\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n    Returns:\n        The sampled token.\n    \"\"\"\nassert self.ctx is not None\nlast_n_tokens_data = [llama_cpp.llama_token(0)] * max(\n0, self.last_n_tokens_size - len(self.eval_tokens)\n) + list(self.eval_tokens)[-self.last_n_tokens_size :]\nreturn llama_cpp.llama_sample_top_p_top_k(\nctx=self.ctx,\nlast_n_tokens_data=(llama_cpp.llama_token * self.last_n_tokens_size)(\n*last_n_tokens_data\n),\nlast_n_tokens_size=llama_cpp.c_int(self.last_n_tokens_size),\ntop_k=llama_cpp.c_int(top_k),\ntop_p=llama_cpp.c_float(top_p),\ntemp=llama_cpp.c_float(temp),\nrepeat_penalty=llama_cpp.c_float(repeat_penalty),\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.generate","title":"<code>generate(tokens, top_k, top_p, temp, repeat_penalty, reset=True)</code>","text":"<p>Create a generator of tokens from a prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n&gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n&gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n...     print(llama.detokenize([token]))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Sequence[llama_cpp.llama_token]</code> <p>The prompt tokens.</p> required <code>top_k</code> <code>int</code> <p>The top-k sampling parameter.</p> required <code>top_p</code> <code>float</code> <p>The top-p sampling parameter.</p> required <code>temp</code> <code>float</code> <p>The temperature parameter.</p> required <code>repeat_penalty</code> <code>float</code> <p>The repeat penalty parameter.</p> required <code>reset</code> <code>bool</code> <p>Whether to reset the model state.</p> <code>True</code> <p>Yields:</p> Type Description <code>Generator[llama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None]</code> <p>The generated tokens.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def generate(\nself,\ntokens: Sequence[llama_cpp.llama_token],\ntop_k: int,\ntop_p: float,\ntemp: float,\nrepeat_penalty: float,\nreset: bool = True,\n) -&gt; Generator[\nllama_cpp.llama_token, Optional[Sequence[llama_cpp.llama_token]], None\n]:\n\"\"\"Create a generator of tokens from a prompt.\n    Examples:\n        &gt;&gt;&gt; llama = Llama(\"models/ggml-7b.bin\")\n        &gt;&gt;&gt; tokens = llama.tokenize(b\"Hello, world!\")\n        &gt;&gt;&gt; for token in llama.generate(tokens, top_k=40, top_p=0.95, temp=1.0, repeat_penalty=1.1):\n        ...     print(llama.detokenize([token]))\n    Args:\n        tokens: The prompt tokens.\n        top_k: The top-k sampling parameter.\n        top_p: The top-p sampling parameter.\n        temp: The temperature parameter.\n        repeat_penalty: The repeat penalty parameter.\n        reset: Whether to reset the model state.\n    Yields:\n        The generated tokens.\n    \"\"\"\nassert self.ctx is not None\nif (\nreset\nand len(self.eval_tokens) &gt; 0\nand tuple(self.eval_tokens) == tuple(tokens[: len(self.eval_tokens)])\n):\nif self.verbose:\nprint(\"generate cache hit\", file=sys.stderr)\nreset = False\ntokens = tokens[len(self.eval_tokens) :]\nif reset:\nself.reset()\nwhile True:\nself.eval(tokens)\ntoken = self.sample(\ntop_k=top_k,\ntop_p=top_p,\ntemp=temp,\nrepeat_penalty=repeat_penalty,\n)\ntokens_or_none = yield token\ntokens = [token]\nif tokens_or_none is not None:\ntokens.extend(tokens_or_none)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_embedding","title":"<code>create_embedding(input)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>Embedding</code> <p>An embedding object.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_embedding(self, input: str) -&gt; Embedding:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        An embedding object.\n    \"\"\"\nassert self.ctx is not None\nif self.params.embedding == False:\nraise RuntimeError(\n\"Llama model must be created with embedding=True to call this method\"\n)\nif self.verbose:\nllama_cpp.llama_reset_timings(self.ctx)\ntokens = self.tokenize(input.encode(\"utf-8\"))\nself.reset()\nself.eval(tokens)\nn_tokens = len(tokens)\nembedding = llama_cpp.llama_get_embeddings(self.ctx)[\n: llama_cpp.llama_n_embd(self.ctx)\n]\nif self.verbose:\nllama_cpp.llama_print_timings(self.ctx)\nreturn {\n\"object\": \"list\",\n\"data\": [\n{\n\"object\": \"embedding\",\n\"embedding\": embedding,\n\"index\": 0,\n}\n],\n\"model\": self.model_path,\n\"usage\": {\n\"prompt_tokens\": n_tokens,\n\"total_tokens\": n_tokens,\n},\n}\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.embed","title":"<code>embed(input)</code>","text":"<p>Embed a string.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>The utf-8 encoded string to embed.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>A list of embeddings</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def embed(self, input: str) -&gt; List[float]:\n\"\"\"Embed a string.\n    Args:\n        input: The utf-8 encoded string to embed.\n    Returns:\n        A list of embeddings\n    \"\"\"\nreturn list(map(float, self.create_embedding(input)[\"data\"][0][\"embedding\"]))\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_completion","title":"<code>create_completion(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], repeat_penalty=1.1, top_k=40, stream=False)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>Optional[List[str]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Iterator[CompletionChunk]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_completion(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\ncompletion_or_chunks = self._create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks\nreturn chunks\ncompletion: Completion = next(completion_or_chunks)  # type: ignore\nreturn completion\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.__call__","title":"<code>__call__(prompt, suffix=None, max_tokens=128, temperature=0.8, top_p=0.95, logprobs=None, echo=False, stop=[], repeat_penalty=1.1, top_k=40, stream=False)</code>","text":"<p>Generate text from a prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The prompt to generate text from.</p> required <code>suffix</code> <code>Optional[str]</code> <p>A suffix to append to the generated text. If None, no suffix is appended.</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>logprobs</code> <code>Optional[int]</code> <p>The number of logprobs to return. If None, no logprobs are returned.</p> <code>None</code> <code>echo</code> <code>bool</code> <p>Whether to echo the prompt.</p> <code>False</code> <code>stop</code> <code>Optional[List[str]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the requested tokens exceed the context window.</p> <code>RuntimeError</code> <p>If the prompt fails to tokenize or the model fails to evaluate the prompt.</p> <p>Returns:</p> Type Description <code>Union[Completion, Iterator[CompletionChunk]]</code> <p>Response object containing the generated text.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def __call__(\nself,\nprompt: str,\nsuffix: Optional[str] = None,\nmax_tokens: int = 128,\ntemperature: float = 0.8,\ntop_p: float = 0.95,\nlogprobs: Optional[int] = None,\necho: bool = False,\nstop: Optional[List[str]] = [],\nrepeat_penalty: float = 1.1,\ntop_k: int = 40,\nstream: bool = False,\n) -&gt; Union[Completion, Iterator[CompletionChunk]]:\n\"\"\"Generate text from a prompt.\n    Args:\n        prompt: The prompt to generate text from.\n        suffix: A suffix to append to the generated text. If None, no suffix is appended.\n        max_tokens: The maximum number of tokens to generate.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        logprobs: The number of logprobs to return. If None, no logprobs are returned.\n        echo: Whether to echo the prompt.\n        stop: A list of strings to stop generation when encountered.\n        repeat_penalty: The penalty to apply to repeated tokens.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n    Raises:\n        ValueError: If the requested tokens exceed the context window.\n        RuntimeError: If the prompt fails to tokenize or the model fails to evaluate the prompt.\n    Returns:\n        Response object containing the generated text.\n    \"\"\"\nreturn self.create_completion(\nprompt=prompt,\nsuffix=suffix,\nmax_tokens=max_tokens,\ntemperature=temperature,\ntop_p=top_p,\nlogprobs=logprobs,\necho=echo,\nstop=stop,\nrepeat_penalty=repeat_penalty,\ntop_k=top_k,\nstream=stream,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.create_chat_completion","title":"<code>create_chat_completion(messages, temperature=0.2, top_p=0.95, top_k=40, stream=False, stop=[], max_tokens=256, repeat_penalty=1.1)</code>","text":"<p>Generate a chat completion from a list of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatCompletionMessage]</code> <p>A list of messages to generate a response for.</p> required <code>temperature</code> <code>float</code> <p>The temperature to use for sampling.</p> <code>0.2</code> <code>top_p</code> <code>float</code> <p>The top-p value to use for sampling.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>The top-k value to use for sampling.</p> <code>40</code> <code>stream</code> <code>bool</code> <p>Whether to stream the results.</p> <code>False</code> <code>stop</code> <code>Optional[List[str]]</code> <p>A list of strings to stop generation when encountered.</p> <code>[]</code> <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> <code>256</code> <code>repeat_penalty</code> <code>float</code> <p>The penalty to apply to repeated tokens.</p> <code>1.1</code> <p>Returns:</p> Type Description <code>Union[ChatCompletion, Iterator[ChatCompletionChunk]]</code> <p>Generated chat completion or a stream of chat completion chunks.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>def create_chat_completion(\nself,\nmessages: List[ChatCompletionMessage],\ntemperature: float = 0.2,\ntop_p: float = 0.95,\ntop_k: int = 40,\nstream: bool = False,\nstop: Optional[List[str]] = [],\nmax_tokens: int = 256,\nrepeat_penalty: float = 1.1,\n) -&gt; Union[ChatCompletion, Iterator[ChatCompletionChunk]]:\n\"\"\"Generate a chat completion from a list of messages.\n    Args:\n        messages: A list of messages to generate a response for.\n        temperature: The temperature to use for sampling.\n        top_p: The top-p value to use for sampling.\n        top_k: The top-k value to use for sampling.\n        stream: Whether to stream the results.\n        stop: A list of strings to stop generation when encountered.\n        max_tokens: The maximum number of tokens to generate.\n        repeat_penalty: The penalty to apply to repeated tokens.\n    Returns:\n        Generated chat completion or a stream of chat completion chunks.\n    \"\"\"\nstop = stop if stop is not None else []\nchat_history = \"\".join(\nf'### {\"Human\" if message[\"role\"] == \"user\" else \"Assistant\"}:{message[\"content\"]}'\nfor message in messages\n)\nPROMPT = chat_history + \"### Assistant:\"\nPROMPT_STOP = [\"### Assistant:\", \"### Human:\"]\ncompletion_or_chunks = self(\nprompt=PROMPT,\nstop=PROMPT_STOP + stop,\ntemperature=temperature,\ntop_p=top_p,\ntop_k=top_k,\nstream=stream,\nmax_tokens=max_tokens,\nrepeat_penalty=repeat_penalty,\n)\nif stream:\nchunks: Iterator[CompletionChunk] = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_chunks_to_chat(chunks)\nelse:\ncompletion: Completion = completion_or_chunks  # type: ignore\nreturn self._convert_text_completion_to_chat(completion)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.set_cache","title":"<code>set_cache(cache)</code>","text":"<p>Set the cache.</p> <p>Parameters:</p> Name Type Description Default <code>cache</code> <code>Optional[LlamaCache]</code> <p>The cache to set.</p> required Source code in <code>llama_cpp/llama.py</code> <pre><code>def set_cache(self, cache: Optional[LlamaCache]):\n\"\"\"Set the cache.\n    Args:\n        cache: The cache to set.\n    \"\"\"\nself.cache = cache\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.save_state","title":"<code>save_state()</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>def save_state(self) -&gt; LlamaState:\nassert self.ctx is not None\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nllama_state = (llama_cpp.c_uint8 * int(state_size))()\nif llama_cpp.llama_copy_state_data(self.ctx, llama_state) != state_size:\nraise RuntimeError(\"Failed to copy llama state data\")\nreturn LlamaState(\neval_tokens=self.eval_tokens.copy(),\neval_logits=self.eval_logits.copy(),\nllama_state=llama_state,\n)\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.load_state","title":"<code>load_state(state)</code>","text":"Source code in <code>llama_cpp/llama.py</code> <pre><code>def load_state(self, state: LlamaState) -&gt; None:\nassert self.ctx is not None\nself.eval_tokens = state.eval_tokens.copy()\nself.eval_logits = state.eval_logits.copy()\nstate_size = llama_cpp.llama_get_state_size(self.ctx)\nif llama_cpp.llama_set_state_data(self.ctx, state.llama_state) != state_size:\nraise RuntimeError(\"Failed to set llama state data\")\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_bos","title":"<code>token_bos()</code>  <code>staticmethod</code>","text":"<p>Return the beginning-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_bos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the beginning-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama.Llama.token_eos","title":"<code>token_eos()</code>  <code>staticmethod</code>","text":"<p>Return the end-of-sequence token.</p> Source code in <code>llama_cpp/llama.py</code> <pre><code>@staticmethod\ndef token_eos() -&gt; llama_cpp.llama_token:\n\"\"\"Return the end-of-sequence token.\"\"\"\nreturn llama_cpp.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_ALL_F32","title":"<code>LLAMA_FTYPE_ALL_F32 = ctypes.c_int(0)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_F16","title":"<code>LLAMA_FTYPE_MOSTLY_F16 = ctypes.c_int(1)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_0","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_0 = ctypes.c_int(2)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_1","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_1 = ctypes.c_int(3)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = ctypes.c_int(4)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTLY_Q4_2","title":"<code>LLAMA_FTYPE_MOSTLY_Q4_2 = ctypes.c_int(5)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTYL_Q4_3","title":"<code>LLAMA_FTYPE_MOSTYL_Q4_3 = ctypes.c_int(6)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTYL_Q5_0","title":"<code>LLAMA_FTYPE_MOSTYL_Q5_0 = ctypes.c_int(8)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTYL_Q5_1","title":"<code>LLAMA_FTYPE_MOSTYL_Q5_1 = ctypes.c_int(9)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.LLAMA_FTYPE_MOSTYL_Q8_0","title":"<code>LLAMA_FTYPE_MOSTYL_Q8_0 = ctypes.c_int(7)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_p","title":"<code>llama_context_p = c_void_p</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params_p","title":"<code>llama_context_params_p = POINTER(llama_context_params)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_progress_callback","title":"<code>llama_progress_callback = ctypes.CFUNCTYPE(None, c_float, c_void_p)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token","title":"<code>llama_token = c_int</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_data_p","title":"<code>llama_token_data_p = POINTER(llama_token_data)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_token_p","title":"<code>llama_token_p = POINTER(llama_token)</code>  <code>module-attribute</code>","text":""},{"location":"#llama_cpp.llama_cpp.llama_context_params","title":"<code>llama_context_params</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_context_params(Structure):\n_fields_ = [\n(\"n_ctx\", c_int),  # text context\n(\"n_parts\", c_int),  # -1 for default\n(\"seed\", c_int),  # RNG seed, 0 for random\n(\"f16_kv\", c_bool),  # use fp16 for KV cache\n(\n\"logits_all\",\nc_bool,\n),  # the llama_eval() call computes all logits, not just the last one\n(\"vocab_only\", c_bool),  # only load the vocabulary, no weights\n(\"use_mmap\", c_bool),  # use mmap if possible\n(\"use_mlock\", c_bool),  # force system to keep model in RAM\n(\"embedding\", c_bool),  # embedding mode only\n# called with a progress value between 0 and 1, pass NULL to disable\n(\"progress_callback\", llama_progress_callback),\n# context pointer passed to the progress callback\n(\"progress_callback_user_data\", c_void_p),\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_data","title":"<code>llama_token_data</code>","text":"<p>         Bases: <code>Structure</code></p> Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>class llama_token_data(Structure):\n_fields_ = [\n(\"id\", llama_token),  # token id\n(\"p\", c_float),  # probability of the token\n(\"plog\", c_float),  # log probability of the token\n]\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_apply_lora_from_file","title":"<code>llama_apply_lora_from_file(ctx, path_lora, path_base_model, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_apply_lora_from_file(\nctx: llama_context_p,\npath_lora: ctypes.c_char_p,\npath_base_model: ctypes.c_char_p,\nn_threads: c_int,\n) -&gt; c_int:\nreturn _lib.llama_apply_lora_from_file(ctx, path_lora, path_base_model, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_context_default_params","title":"<code>llama_context_default_params()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_context_default_params() -&gt; llama_context_params:\nreturn _lib.llama_context_default_params()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_copy_state_data","title":"<code>llama_copy_state_data(ctx, dest)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_copy_state_data(ctx: llama_context_p, dest) -&gt; c_size_t:\nreturn _lib.llama_copy_state_data(ctx, dest)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_eval","title":"<code>llama_eval(ctx, tokens, n_tokens, n_past, n_threads)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_eval(\nctx: llama_context_p,\ntokens,  # type: Array[llama_token]\nn_tokens: c_int,\nn_past: c_int,\nn_threads: c_int,\n) -&gt; c_int:\nreturn _lib.llama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_free","title":"<code>llama_free(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_free(ctx: llama_context_p):\n_lib.llama_free(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_embeddings","title":"<code>llama_get_embeddings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_embeddings(ctx: llama_context_p):\nreturn _lib.llama_get_embeddings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_kv_cache_token_count","title":"<code>llama_get_kv_cache_token_count(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_kv_cache_token_count(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_get_kv_cache_token_count(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_logits","title":"<code>llama_get_logits(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_logits(ctx: llama_context_p):\nreturn _lib.llama_get_logits(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_get_state_size","title":"<code>llama_get_state_size(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_get_state_size(ctx: llama_context_p) -&gt; c_size_t:\nreturn _lib.llama_get_state_size(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_init_from_file","title":"<code>llama_init_from_file(path_model, params)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_init_from_file(\npath_model: bytes, params: llama_context_params\n) -&gt; llama_context_p:\nreturn _lib.llama_init_from_file(path_model, params)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_load_session_file","title":"<code>llama_load_session_file(ctx, path_session, tokens_out, n_token_capacity, n_token_count_out)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_load_session_file(\nctx: llama_context_p,\npath_session: bytes,\ntokens_out,\nn_token_capacity: c_size_t,\nn_token_count_out,\n) -&gt; c_size_t:\nreturn _lib.llama_load_session_file(\nctx, path_session, tokens_out, n_token_capacity, n_token_count_out\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_mlock_supported","title":"<code>llama_mlock_supported()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_mlock_supported() -&gt; c_bool:\nreturn _lib.llama_mlock_supported()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_mmap_supported","title":"<code>llama_mmap_supported()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_mmap_supported() -&gt; c_bool:\nreturn _lib.llama_mmap_supported()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_model_quantize","title":"<code>llama_model_quantize(fname_inp, fname_out, ftype, nthread)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_model_quantize(\nfname_inp: bytes, fname_out: bytes, ftype: c_int, nthread: c_int\n) -&gt; c_int:\nreturn _lib.llama_model_quantize(fname_inp, fname_out, ftype, nthread)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_ctx","title":"<code>llama_n_ctx(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_ctx(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_ctx(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_embd","title":"<code>llama_n_embd(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_embd(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_embd(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_n_vocab","title":"<code>llama_n_vocab(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_n_vocab(ctx: llama_context_p) -&gt; c_int:\nreturn _lib.llama_n_vocab(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_system_info","title":"<code>llama_print_system_info()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_system_info() -&gt; bytes:\nreturn _lib.llama_print_system_info()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_print_timings","title":"<code>llama_print_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_print_timings(ctx: llama_context_p):\n_lib.llama_print_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_reset_timings","title":"<code>llama_reset_timings(ctx)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_reset_timings(ctx: llama_context_p):\n_lib.llama_reset_timings(ctx)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_sample_top_p_top_k","title":"<code>llama_sample_top_p_top_k(ctx, last_n_tokens_data, last_n_tokens_size, top_k, top_p, temp, repeat_penalty)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_sample_top_p_top_k(\nctx: llama_context_p,\nlast_n_tokens_data,  # type: Array[llama_token]\nlast_n_tokens_size: c_int,\ntop_k: c_int,\ntop_p: c_float,\ntemp: c_float,\nrepeat_penalty: c_float,\n) -&gt; llama_token:\nreturn _lib.llama_sample_top_p_top_k(\nctx, last_n_tokens_data, last_n_tokens_size, top_k, top_p, temp, repeat_penalty\n)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_save_session_file","title":"<code>llama_save_session_file(ctx, path_session, tokens, n_token_count)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_save_session_file(\nctx: llama_context_p, path_session: bytes, tokens, n_token_count: c_size_t\n) -&gt; c_size_t:\nreturn _lib.llama_save_session_file(ctx, path_session, tokens, n_token_count)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_rng_seed","title":"<code>llama_set_rng_seed(ctx, seed)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_rng_seed(ctx: llama_context_p, seed: c_int):\nreturn _lib.llama_set_rng_seed(ctx, seed)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_set_state_data","title":"<code>llama_set_state_data(ctx, src)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_set_state_data(ctx: llama_context_p, src) -&gt; c_size_t:\nreturn _lib.llama_set_state_data(ctx, src)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_bos","title":"<code>llama_token_bos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_bos() -&gt; llama_token:\nreturn _lib.llama_token_bos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_eos","title":"<code>llama_token_eos()</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_eos() -&gt; llama_token:\nreturn _lib.llama_token_eos()\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_token_to_str","title":"<code>llama_token_to_str(ctx, token)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_token_to_str(ctx: llama_context_p, token: llama_token) -&gt; bytes:\nreturn _lib.llama_token_to_str(ctx, token)\n</code></pre>"},{"location":"#llama_cpp.llama_cpp.llama_tokenize","title":"<code>llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)</code>","text":"Source code in <code>llama_cpp/llama_cpp.py</code> <pre><code>def llama_tokenize(\nctx: llama_context_p,\ntext: bytes,\ntokens,  # type: Array[llama_token]\nn_max_tokens: c_int,\nadd_bos: c_bool,\n) -&gt; c_int:\nreturn _lib.llama_tokenize(ctx, text, tokens, n_max_tokens, add_bos)\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the MIT license.</p>"}]}